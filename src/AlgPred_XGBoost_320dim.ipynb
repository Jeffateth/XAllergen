{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "import torch\n",
    "import esm\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import csv\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import zipfile\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User input: choose which dataset to evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose between 'benchmark' or 'algpred2' for now only those two\n",
    "# dataset_name = \"benchmark\"\n",
    "dataset_name = \"algpred2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == \"benchmark\":\n",
    "    # Define base folder and output paths\n",
    "    data_dir = dataset_name\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    train_csv_path = os.path.join(data_dir, f\"{dataset_name}_train.csv\")\n",
    "    test_csv_path = os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n",
    "\n",
    "    # Check for existing files\n",
    "    if os.path.exists(train_csv_path) or os.path.exists(test_csv_path):\n",
    "        user_input = (\n",
    "            input(\n",
    "                f\"⚠️ Files already exist in '{data_dir}/'. Do you want to overwrite them? (y/n): \"\n",
    "            )\n",
    "            .strip()\n",
    "            .lower()\n",
    "        )\n",
    "        if user_input != \"y\":\n",
    "            print(\"⏭️  Skipping FASTA parsing and CSV generation.\")\n",
    "        else:\n",
    "            proceed = True\n",
    "    else:\n",
    "        proceed = True\n",
    "\n",
    "    if \"proceed\" in locals() and proceed:\n",
    "        # Define file-label mapping and GitHub raw URLs\n",
    "        base_url = \"https://raw.githubusercontent.com/Jeffateth/AllergenPredict/7fafbea0ab1646796abe40cafb800c46ba842bda/Benchmark_dataset\"\n",
    "\n",
    "        datasets = {\n",
    "            \"train_p.fasta\": (1, \"train\"),\n",
    "            \"train_n.fasta\": (0, \"train\"),\n",
    "            \"test_p.fasta\": (1, \"test\"),\n",
    "            \"test_n.fasta\": (0, \"test\"),\n",
    "        }\n",
    "\n",
    "        # Parse FASTA format\n",
    "        def parse_fasta(fasta_text, label):\n",
    "            sequences = []\n",
    "            current_id = None\n",
    "            current_seq = \"\"\n",
    "            for line in fasta_text.strip().splitlines():\n",
    "                line = line.strip()\n",
    "                if line.startswith(\">\"):\n",
    "                    if current_id is not None:\n",
    "                        sequences.append((current_id, current_seq, label))\n",
    "                    current_id = line[1:]\n",
    "                    current_seq = \"\"\n",
    "                else:\n",
    "                    current_seq += line\n",
    "            if current_id and current_seq:\n",
    "                sequences.append((current_id, current_seq, label))\n",
    "            return sequences\n",
    "\n",
    "        # Download and parse files\n",
    "        train_entries = []\n",
    "        test_entries = []\n",
    "\n",
    "        for filename, (label, split) in datasets.items():\n",
    "            url = f\"{base_url}/{filename}\"\n",
    "            print(f\"⬇️  Downloading {filename} from {url}...\")\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # raise an error for failed downloads\n",
    "\n",
    "            fasta_text = response.text\n",
    "            entries = parse_fasta(fasta_text, label)\n",
    "            if split == \"train\":\n",
    "                train_entries.extend(entries)\n",
    "            else:\n",
    "                test_entries.extend(entries)\n",
    "\n",
    "        # Save to CSV inside dataset-named folder\n",
    "        df_train = pd.DataFrame(train_entries, columns=[\"id\", \"sequence\", \"label\"])\n",
    "        df_test = pd.DataFrame(test_entries, columns=[\"id\", \"sequence\", \"label\"])\n",
    "\n",
    "        df_train.to_csv(train_csv_path, index=False)\n",
    "        df_test.to_csv(test_csv_path, index=False)\n",
    "\n",
    "        print(f\"✅ Saved training set to '{train_csv_path}'\")\n",
    "        print(f\"✅ Saved testing set to '{test_csv_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data from AllergenAI (need to update this code to fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Standard amino acid order (1-letter codes)\n",
    "# aa_letters = list(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "\n",
    "# # Map one-hot vector to amino acid letter\n",
    "# onehot_to_aa = {\n",
    "#     tuple(1 if i == j else 0 for i in range(20)): aa\n",
    "#     for j, aa in enumerate(aa_letters)\n",
    "# }\n",
    "\n",
    "# def load_onehot_file(filepath, label):\n",
    "#     \"\"\"Converts one-hot file to list of (sequence, label)\"\"\"\n",
    "#     data = np.loadtxt(filepath)\n",
    "#     sequences = []\n",
    "#     current = []\n",
    "\n",
    "#     for row in data:\n",
    "#         if np.all(row == 0):\n",
    "#             if current:\n",
    "#                 sequences.append((\"\".join(current), label))\n",
    "#                 current = []\n",
    "#         else:\n",
    "#             aa = onehot_to_aa.get(tuple(int(x) for x in row))\n",
    "#             if aa:\n",
    "#                 current.append(aa)\n",
    "#             else:\n",
    "#                 raise ValueError(f\"Unknown one-hot vector: {row}\")\n",
    "\n",
    "#     if current:\n",
    "#         sequences.append((\"\".join(current), label))\n",
    "\n",
    "#     return sequences\n",
    "\n",
    "# # === Load both files ===\n",
    "# positive_sequences = load_onehot_file(\"pos.txt\", label=1)\n",
    "# negative_sequences = load_onehot_file(\"neg.txt\", label=0)\n",
    "\n",
    "# # === Combine and save ===\n",
    "# all_sequences = positive_sequences + negative_sequences\n",
    "# df = pd.DataFrame(all_sequences, columns=[\"sequence\", \"label\"])\n",
    "# df[\"id\"] = range(len(df))\n",
    "\n",
    "# # Reorder columns if you want: id, sequence, label\n",
    "# df = df[[\"id\", \"sequence\", \"label\"]]\n",
    "\n",
    "# # Save it back\n",
    "# df.to_csv(\"converted_onehot_sequences.csv\", index=False)\n",
    "# print(\"✅ Saved as 'converted_onehot_sequences.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data from AlgPred 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9444,
     "status": "ok",
     "timestamp": 1743545518910,
     "user": {
      "displayName": "Jeff",
      "userId": "15773939950998775573"
     },
     "user_tz": -120
    },
    "id": "ilcsg8tr0Nus",
    "outputId": "6b11ab8a-9c74-4ba9-f7fb-5b59a4090162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️  Downloading train_positive from https://webs.iiitd.edu.in/raghava/algpred2/datasets/train_positive.txt...\n",
      "⬇️  Downloading train_negative from https://webs.iiitd.edu.in/raghava/algpred2/datasets/train_negative.txt...\n",
      "⬇️  Downloading validation_positive from https://webs.iiitd.edu.in/raghava/algpred2/datasets/validation_positive.txt...\n",
      "⬇️  Downloading validation_negative from https://webs.iiitd.edu.in/raghava/algpred2/datasets/validation_negative.txt...\n",
      "✅ Saved training set to 'algpred2/algpred2_train.csv'\n",
      "✅ Saved validation set to 'algpred2/algpred2_test.csv'\n"
     ]
    }
   ],
   "source": [
    "if dataset_name == \"algpred2\":\n",
    "    # Define output paths inside dataset-named directory\n",
    "    data_dir = dataset_name\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    train_csv_path = os.path.join(data_dir, f\"{dataset_name}_train.csv\")\n",
    "    test_csv_path = os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n",
    "\n",
    "    # Check if files already exist\n",
    "    if os.path.exists(train_csv_path) or os.path.exists(test_csv_path):\n",
    "        user_input = (\n",
    "            input(\n",
    "                f\"⚠️ Files already exist in '{data_dir}/'. Do you want to overwrite them? (y/n): \"\n",
    "            )\n",
    "            .strip()\n",
    "            .lower()\n",
    "        )\n",
    "        if user_input != \"y\":\n",
    "            print(\"⏭️  Skipping download and parsing.\")\n",
    "        else:\n",
    "            proceed = True\n",
    "    else:\n",
    "        proceed = True\n",
    "\n",
    "    if \"proceed\" in locals() and proceed:\n",
    "        # URLs from AlgPred 2.0\n",
    "        datasets = {\n",
    "            \"train_positive\": (\n",
    "                \"https://webs.iiitd.edu.in/raghava/algpred2/datasets/train_positive.txt\",\n",
    "                1,\n",
    "                \"train\",\n",
    "            ),\n",
    "            \"train_negative\": (\n",
    "                \"https://webs.iiitd.edu.in/raghava/algpred2/datasets/train_negative.txt\",\n",
    "                0,\n",
    "                \"train\",\n",
    "            ),\n",
    "            \"validation_positive\": (\n",
    "                \"https://webs.iiitd.edu.in/raghava/algpred2/datasets/validation_positive.txt\",\n",
    "                1,\n",
    "                \"val\",\n",
    "            ),\n",
    "            \"validation_negative\": (\n",
    "                \"https://webs.iiitd.edu.in/raghava/algpred2/datasets/validation_negative.txt\",\n",
    "                0,\n",
    "                \"val\",\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Parse FASTA format\n",
    "        def parse_fasta(fasta_text, label):\n",
    "            sequences = []\n",
    "            current_id = None\n",
    "            current_seq = \"\"\n",
    "            for line in fasta_text.strip().splitlines():\n",
    "                line = line.strip()\n",
    "                if line.startswith(\">\"):\n",
    "                    if current_id is not None:\n",
    "                        sequences.append((current_id, current_seq, label))\n",
    "                    current_id = line[1:]  # remove \">\"\n",
    "                    current_seq = \"\"\n",
    "                else:\n",
    "                    current_seq += line\n",
    "            if current_id and current_seq:\n",
    "                sequences.append((current_id, current_seq, label))\n",
    "            return sequences\n",
    "\n",
    "        # Split into train and validation entries\n",
    "        train_entries = []\n",
    "        val_entries = []\n",
    "\n",
    "        for name, (url, label, split) in datasets.items():\n",
    "            print(f\"⬇️  Downloading {name} from {url}...\")\n",
    "            response = requests.get(url)\n",
    "            entries = parse_fasta(response.text, label)\n",
    "            if split == \"train\":\n",
    "                train_entries.extend(entries)\n",
    "            else:\n",
    "                val_entries.extend(entries)\n",
    "\n",
    "        # Convert to DataFrames\n",
    "        df_train = pd.DataFrame(train_entries, columns=[\"id\", \"sequence\", \"label\"])\n",
    "        df_val = pd.DataFrame(val_entries, columns=[\"id\", \"sequence\", \"label\"])\n",
    "\n",
    "        # Save to CSV inside dataset folder\n",
    "        df_train.to_csv(train_csv_path, index=False)\n",
    "        df_val.to_csv(test_csv_path, index=False)\n",
    "\n",
    "        print(f\"✅ Saved training set to '{train_csv_path}'\")\n",
    "        print(f\"✅ Saved validation set to '{test_csv_path}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ESM-2 embedding extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 206837,
     "status": "ok",
     "timestamp": 1743546270778,
     "user": {
      "displayName": "Jeff",
      "userId": "15773939950998775573"
     },
     "user_tz": -120
    },
    "id": "1_Jv62ty0mrA",
    "outputId": "778a1e22-2458-4988-a7f1-e5b0c49097ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️  Extracting embeddings for train set... (16120 sequences remaining)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16120/16120 [25:20<00:00, 10.60it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final train embeddings saved to 'algpred2/train_algpred2_esm2_embeddings.csv'\n",
      "⚙️  Extracting embeddings for test set... (4030 sequences remaining)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4030/4030 [06:08<00:00, 10.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final test embeddings saved to 'algpred2/test_algpred2_esm2_embeddings.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# === CONFIG ===\n",
    "feature_dim = 320  # ESM-2 T6-8M embedding size\n",
    "batch_size = 1  # Adjust based on memory\n",
    "data_dir = dataset_name  # All files live in a folder named after the dataset\n",
    "\n",
    "# --- Ensure directory exists ---\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# --- Construct dynamic file paths ---\n",
    "input_files = {\n",
    "    \"train\": os.path.join(data_dir, f\"{dataset_name}_train.csv\"),\n",
    "    \"test\": os.path.join(data_dir, f\"{dataset_name}_test.csv\"),\n",
    "}\n",
    "\n",
    "# --- Output file paths ---\n",
    "embedding_files = {\n",
    "    \"train\": os.path.join(data_dir, f\"train_{dataset_name}_esm2_embeddings.csv\"),\n",
    "    \"test\": os.path.join(data_dir, f\"test_{dataset_name}_esm2_embeddings.csv\"),\n",
    "}\n",
    "\n",
    "# Check if both embedding files exist\n",
    "if all(os.path.exists(f) for f in embedding_files.values()):\n",
    "    print(\n",
    "        f\"✅ ESM2 embedding files already exist in '{data_dir}/'. Skipping embedding generation.\"\n",
    "    )\n",
    "else:\n",
    "    # --- Load ESM-2 model ---\n",
    "    model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n",
    "    batch_converter = alphabet.get_batch_converter()\n",
    "    model.eval()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # --- Helper function ---\n",
    "    def process_file(split_name, input_file):\n",
    "        temp_file = os.path.join(\n",
    "            data_dir, f\"{split_name}_{dataset_name}_esm2_embeddings_temp.csv\"\n",
    "        )\n",
    "        final_file = os.path.join(\n",
    "            data_dir, f\"{split_name}_{dataset_name}_esm2_embeddings.csv\"\n",
    "        )\n",
    "\n",
    "        # Load dataset\n",
    "        df = pd.read_csv(input_file)\n",
    "        sequences = list(df[\"sequence\"])\n",
    "        labels = list(df[\"label\"])\n",
    "        ids = list(df[\"id\"])\n",
    "\n",
    "        # Resume support\n",
    "        if os.path.exists(temp_file):\n",
    "            processed_ids = set(pd.read_csv(temp_file, usecols=[\"id\"])[\"id\"])\n",
    "            print(\n",
    "                f\"🔁 Resuming {split_name} from {temp_file} — {len(processed_ids)} entries already processed.\"\n",
    "            )\n",
    "        else:\n",
    "            processed_ids = set()\n",
    "\n",
    "        remaining_data = [\n",
    "            (ids[i], sequences[i], labels[i])\n",
    "            for i in range(len(ids))\n",
    "            if ids[i] not in processed_ids\n",
    "        ]\n",
    "\n",
    "        # Output format\n",
    "        fieldnames = [\"id\", \"label\"] + [f\"f{k}\" for k in range(feature_dim)]\n",
    "        write_header = not os.path.exists(temp_file)\n",
    "\n",
    "        print(\n",
    "            f\"⚙️  Extracting embeddings for {split_name} set... ({len(remaining_data)} sequences remaining)\"\n",
    "        )\n",
    "\n",
    "        with open(temp_file, mode=\"a\", newline=\"\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "            if write_header:\n",
    "                writer.writeheader()\n",
    "\n",
    "            for i in tqdm(range(0, len(remaining_data), batch_size)):\n",
    "                batch = remaining_data[i : i + batch_size]\n",
    "                batch_ids = [x[0] for x in batch]\n",
    "                batch_seqs = [x[1] for x in batch]\n",
    "                batch_labels = [x[2] for x in batch]\n",
    "\n",
    "                batch_data = [\n",
    "                    (batch_ids[j], batch_seqs[j]) for j in range(len(batch_seqs))\n",
    "                ]\n",
    "                _, _, batch_tokens = batch_converter(batch_data)\n",
    "                batch_tokens = batch_tokens.to(device)\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(batch_tokens, repr_layers=[6])\n",
    "                    token_representations = outputs[\"representations\"][6]\n",
    "\n",
    "                rows = []\n",
    "                for j, (_, seq) in enumerate(batch_data):\n",
    "                    representation = token_representations[j, 1 : len(seq) + 1].mean(0)\n",
    "                    entry = {\n",
    "                        \"id\": batch_ids[j],\n",
    "                        \"label\": batch_labels[j],\n",
    "                    }\n",
    "                    for k in range(feature_dim):\n",
    "                        entry[f\"f{k}\"] = representation[k].item()\n",
    "                    rows.append(entry)\n",
    "\n",
    "                writer.writerows(rows)\n",
    "\n",
    "        # Final save\n",
    "        os.replace(temp_file, final_file)\n",
    "        print(f\"✅ Final {split_name} embeddings saved to '{final_file}'\")\n",
    "\n",
    "    # --- Process each split ---\n",
    "    for split, file in input_files.items():\n",
    "        process_file(split, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (XGBoosted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101704,
     "status": "ok",
     "timestamp": 1743547169212,
     "user": {
      "displayName": "Jeff",
      "userId": "15773939950998775573"
     },
     "user_tz": -120
    },
    "id": "dvQ6gYSmeMsx",
    "outputId": "9e230e2d-17e6-48b8-c75a-c17026d7c36f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded: Train=(16120, 320), Test=(4030, 320)\n",
      "\n",
      "📉 DummyClassifier (Stratified) on Training Set (CV):\n",
      "\n",
      "📊 Dummy ROC-AUC: 0.4991 ± 0.0000\n",
      "\n",
      "🚀 5-Fold Cross-Validation (XGBoost) on Training Set...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Fold 1 AUC: 0.9957\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9818    0.9708    0.9763      1612\n",
      "           1     0.9712    0.9820    0.9766      1612\n",
      "\n",
      "    accuracy                         0.9764      3224\n",
      "   macro avg     0.9765    0.9764    0.9764      3224\n",
      "weighted avg     0.9765    0.9764    0.9764      3224\n",
      "\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Fold 2 AUC: 0.9972\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9801    0.9764    0.9782      1612\n",
      "           1     0.9765    0.9801    0.9783      1612\n",
      "\n",
      "    accuracy                         0.9783      3224\n",
      "   macro avg     0.9783    0.9783    0.9783      3224\n",
      "weighted avg     0.9783    0.9783    0.9783      3224\n",
      "\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Fold 3 AUC: 0.9978\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9838    0.9789    0.9813      1612\n",
      "           1     0.9790    0.9839    0.9814      1612\n",
      "\n",
      "    accuracy                         0.9814      3224\n",
      "   macro avg     0.9814    0.9814    0.9814      3224\n",
      "weighted avg     0.9814    0.9814    0.9814      3224\n",
      "\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Fold 4 AUC: 0.9958\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9795    0.9789    0.9792      1612\n",
      "           1     0.9789    0.9795    0.9792      1612\n",
      "\n",
      "    accuracy                         0.9792      3224\n",
      "   macro avg     0.9792    0.9792    0.9792      3224\n",
      "weighted avg     0.9792    0.9792    0.9792      3224\n",
      "\n",
      "------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Fold 5 AUC: 0.9976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9855    0.9671    0.9762      1612\n",
      "           1     0.9677    0.9857    0.9766      1612\n",
      "\n",
      "    accuracy                         0.9764      3224\n",
      "   macro avg     0.9766    0.9764    0.9764      3224\n",
      "weighted avg     0.9766    0.9764    0.9764      3224\n",
      "\n",
      "------\n",
      "\n",
      "✅ Mean CV ROC-AUC: 0.9968 ± 0.0009\n",
      "\n",
      "🔒 Final Evaluation on Hold-Out Test Set...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7369    0.9811    0.8416      2015\n",
      "           1     0.9718    0.6496    0.7787      2015\n",
      "\n",
      "    accuracy                         0.8154      4030\n",
      "   macro avg     0.8543    0.8154    0.8102      4030\n",
      "weighted avg     0.8543    0.8154    0.8102      4030\n",
      "\n",
      "🎯 Final Test ROC-AUC: 0.9477\n",
      "\n",
      "🧪 Y-Scrambling (sanity check) on Training Set...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n",
      "/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔀 Y-Scrambled ROC-AUC: 0.4974 ± 0.0093\n",
      "👉 This should be near 0.5 if your real model learned something.\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# Step 1: Load Data\n",
    "# ====================================\n",
    "# Set the dataset name\n",
    "\n",
    "# Construct embedding file paths based on dataset name\n",
    "embedding_files = {\n",
    "    \"train\": f\"train_{dataset_name}_esm2_embeddings.csv\",\n",
    "    \"test\": f\"test_{dataset_name}_esm2_embeddings.csv\",\n",
    "}\n",
    "\n",
    "# Load the data\n",
    "df_train = pd.read_csv(embedding_files[\"train\"])\n",
    "df_test = pd.read_csv(embedding_files[\"test\"])\n",
    "\n",
    "\n",
    "feature_cols = [f\"f{i}\" for i in range(320)]\n",
    "X_train_full = df_train[feature_cols].values\n",
    "y_train_full = df_train[\"label\"].values\n",
    "\n",
    "X_test = df_test[feature_cols].values\n",
    "y_test = df_test[\"label\"].values\n",
    "\n",
    "print(f\"✅ Loaded: Train={X_train_full.shape}, Test={X_test.shape}\")\n",
    "\n",
    "# ====================================\n",
    "# Step 2: Dummy Classifier Baseline (on Train)\n",
    "# ====================================\n",
    "print(\"\\n📉 DummyClassifier (Stratified) on Training Set (CV):\\n\")\n",
    "dummy = DummyClassifier(strategy=\"stratified\", random_state=42)\n",
    "dummy_aucs = []\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for train_idx, val_idx in cv.split(X_train_full, y_train_full):\n",
    "    dummy.fit(X_train_full[train_idx], y_train_full[train_idx])\n",
    "    y_dummy_proba = dummy.predict_proba(X_train_full[val_idx])[:, 1]\n",
    "    auc = roc_auc_score(y_train_full[val_idx], y_dummy_proba)\n",
    "    dummy_aucs.append(auc)\n",
    "\n",
    "print(f\"📊 Dummy ROC-AUC: {np.mean(dummy_aucs):.4f} ± {np.std(dummy_aucs):.4f}\")\n",
    "\n",
    "# ====================================\n",
    "# Step 3: Cross-Validation on Training Set (XGBoost)\n",
    "# ====================================\n",
    "print(\"\\n🚀 5-Fold Cross-Validation (XGBoost) on Training Set...\\n\")\n",
    "xgb_aucs = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_full, y_train_full)):\n",
    "    X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]\n",
    "    y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n",
    "\n",
    "    clf = xgb.XGBClassifier(\n",
    "        use_label_encoder=False, eval_metric=\"logloss\", random_state=42\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_val)\n",
    "    y_proba = clf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_proba)\n",
    "    xgb_aucs.append(auc)\n",
    "\n",
    "    print(f\"📂 Fold {fold+1} AUC: {auc:.4f}\")\n",
    "    print(classification_report(y_val, y_pred, digits=4))\n",
    "    print(\"------\")\n",
    "\n",
    "print(f\"\\n✅ Mean CV ROC-AUC: {np.mean(xgb_aucs):.4f} ± {np.std(xgb_aucs):.4f}\")\n",
    "\n",
    "# ====================================\n",
    "# Step 4: Final Test Set Evaluation\n",
    "# ====================================\n",
    "print(\"\\n🔒 Final Evaluation on Hold-Out Test Set...\\n\")\n",
    "clf_final = xgb.XGBClassifier(\n",
    "    use_label_encoder=False, eval_metric=\"logloss\", random_state=42\n",
    ")\n",
    "clf_final.fit(X_train_full, y_train_full)\n",
    "\n",
    "y_test_pred = clf_final.predict(X_test)\n",
    "y_test_proba = clf_final.predict_proba(X_test)[:, 1]\n",
    "\n",
    "test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "print(classification_report(y_test, y_test_pred, digits=4))\n",
    "print(f\"🎯 Final Test ROC-AUC: {test_auc:.4f}\")\n",
    "\n",
    "# ====================================\n",
    "# Step 5: Y-Scrambling Control\n",
    "# ====================================\n",
    "print(\"\\n🧪 Y-Scrambling (sanity check) on Training Set...\\n\")\n",
    "y_scrambled = y_train_full.copy()\n",
    "random.seed(42)\n",
    "random.shuffle(y_scrambled)\n",
    "\n",
    "scrambled_aucs = []\n",
    "for train_idx, val_idx in cv.split(X_train_full, y_scrambled):\n",
    "    X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]\n",
    "    y_train, y_val = y_scrambled[train_idx], y_scrambled[val_idx]\n",
    "\n",
    "    clf_scrambled = xgb.XGBClassifier(\n",
    "        use_label_encoder=False, eval_metric=\"logloss\", random_state=42\n",
    "    )\n",
    "    clf_scrambled.fit(X_train, y_train)\n",
    "    y_proba_scrambled = clf_scrambled.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    auc = roc_auc_score(y_val, y_proba_scrambled)\n",
    "    scrambled_aucs.append(auc)\n",
    "\n",
    "print(\n",
    "    f\"🔀 Y-Scrambled ROC-AUC: {np.mean(scrambled_aucs):.4f} ± {np.std(scrambled_aucs):.4f}\"\n",
    ")\n",
    "print(\"👉 This should be near 0.5 if your real model learned something.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyO8zzovwpYg5elCgDfk9yWE",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
