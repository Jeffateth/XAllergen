{"cells":[{"cell_type":"markdown","metadata":{},"source":["## imports"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["#esm2_env\n","import pandas as pd\n","import numpy as np\n","import requests\n","from io import StringIO\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","import esm\n","from tqdm import tqdm\n","import sys\n","import os\n","import csv\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","from sklearn.metrics import roc_auc_score, classification_report\n","from sklearn.dummy import DummyClassifier\n","from sklearn.linear_model import RidgeClassifier\n","import xgboost as xgb\n","import random\n","import zipfile\n","import io\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","import time\n","from contextlib import redirect_stdout, redirect_stderr\n","from transformers import BertModel, BertTokenizer\n","import joblib\n","\n","######################\n","# REQUIRED PACKAGES\n","# pandas\n","# numpy\n","# requests\n","# torch\n","# esm\n","# tqdm\n","# scikit-learn\n","# xgboost\n","# ipywidgets\n","# transformers\n","#####################"]},{"cell_type":"markdown","metadata":{},"source":["## User input: Dataset "]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"faf170dc99f64296a4f09ea80b3b2a3b","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Dropdown(description='Select Dataset:', layout=Layout(width='350px'), options=('algpred2', 'alg…"]},"metadata":{},"output_type":"display_data"}],"source":["# IMPORTANT: \n","# 1. run the import cell and this one. \n","# 2. after choosing and pressing confirm in the widget run the rest of the code (run + arrow down in the cell options of the next cell)\n","# === Layout ===\n","layout = widgets.Layout(width='350px')\n","style = {'description_width': '150px'}\n","\n","# === Dropdowns ===\n","dataset_selector = widgets.Dropdown(\n","    options=[\"algpred2\", \"algpred2_resplit\", \"iedb\", \"AllergenAI\"],\n","    value=\"algpred2\",\n","    description=\"Select Dataset:\",\n","    layout=layout,\n","    style=style\n",")\n","\n","transformer_selector = widgets.Dropdown(\n","    options=[\"ESM-2_320dim\",\"ProtBert_1024dim\",\"ProtT5_1024dim\"],\n","    value=\"ESM-2_320dim\",\n","    description=\"Select Transformer:\",\n","    layout=layout,\n","    style=style\n",")\n","\n","model_selector = widgets.Dropdown(\n","    options=[\"XGBoost\", \"FFNN\",\"Ridge\"],\n","    value=\"XGBoost\",\n","    description=\"Select Model:\",\n","    layout=layout,\n","    style=style\n",")\n","\n","# === Output + Button ===\n","output = widgets.Output()\n","submit_button = widgets.Button(description=\"✅ Confirm Selection\", button_style='success')\n","\n","# === Button callback ===\n","def on_button_clicked(b):\n","    global dataset_name, transformer_name, model_name  # <-- Add this\n","    dataset_name = dataset_selector.value\n","    transformer_name = transformer_selector.value\n","    model_name = model_selector.value\n","\n","    with output:\n","        clear_output()\n","        print(\"✅ Selections made!\")\n","        print(f\"Dataset: {dataset_name}\")\n","        print(f\"Transformer: {transformer_name}\")\n","        print(f\"Model: {model_name}\")\n","\n","\n","submit_button.on_click(on_button_clicked)\n","\n","# === Display UI ===\n","display(widgets.VBox([\n","    dataset_selector,\n","    transformer_selector,\n","    model_selector,\n","    submit_button,\n","    output\n","]))\n"]},{"cell_type":"markdown","metadata":{},"source":["# Data curation"]},{"cell_type":"markdown","metadata":{},"source":["## Data from IEDB"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["IEDB dataset not chosen\n"]}],"source":["# if dataset_name == 'iedb':\n","#     print('IEDB dataset chosen')\n","\n","#     # === CONFIG ===\n","#     data_dir = dataset_name\n","#     os.makedirs(data_dir, exist_ok=True)\n","\n","#     # --- Load original CSV ---\n","#     url = \"https://raw.githubusercontent.com/Jeffateth/AllergenPredict/b395c3276945b83ecc77513749361d6472706ca5/allergen_data_with_full_sequences.csv\"\n","#     df = pd.read_csv(url)\n","\n","#     # --- Prepare DataFrame ---\n","#     df = df[[\"full_parent_protein_sequence\", \"label\"]].copy()\n","#     df.rename(columns={\"full_parent_protein_sequence\": \"sequence\"}, inplace=True)\n","#     df[\"id\"] = [f\"seq_{i}\" for i in range(len(df))]\n","#     df = df[[\"id\", \"sequence\", \"label\"]]\n","\n","#     # --- Train/test split (80/20 stratified) ---\n","#     train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n","\n","#     # --- Save splits ---\n","#     train_df.to_csv(os.path.join(data_dir, f\"{dataset_name}_train.csv\"), index=False)\n","#     test_df.to_csv(os.path.join(data_dir, f\"{dataset_name}_test.csv\"), index=False)\n","\n","#     print(\"✅ Data loaded, split, and saved for ESM2 embedding.\")\n","# else: print('IEDB dataset not chosen')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data from AllergenAI "]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["AllergenAI dataset not chosen\n"]}],"source":["# if dataset_name == 'AllergenAI':\n","#     print('AllergenAI dataset chosen')\n","\n","#     # === CONFIG ===\n","#     data_dir = dataset_name\n","#     os.makedirs(data_dir, exist_ok=True)\n","\n","#     train_csv_path = os.path.join(data_dir, f\"{dataset_name}_train.csv\")\n","#     test_csv_path = os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","\n","#     # === Skip if already processed\n","#     if os.path.exists(train_csv_path) and os.path.exists(test_csv_path):\n","#         print(f\"⏭️  Found existing train/test files in '{data_dir}/'. Skipping parsing.\")\n","#     else:\n","#         # === Standard amino acid order (1-letter codes)\n","#         aa_letters = list(\"ACDEFGHIKLMNPQRSTVWY\")\n","\n","#         # === Map one-hot vector to amino acid letter\n","#         onehot_to_aa = {\n","#             tuple(1 if i == j else 0 for i in range(20)): aa\n","#             for j, aa in enumerate(aa_letters)\n","#         }\n","\n","#         def load_onehot_file(filepath, label):\n","#             \"\"\"Converts one-hot file to list of (sequence, label)\"\"\"\n","#             data = np.loadtxt(filepath)\n","#             sequences = []\n","#             current = []\n","\n","#             for row in data:\n","#                 if np.all(row == 0):\n","#                     if current:\n","#                         sequences.append((\"\".join(current), label))\n","#                         current = []\n","#                 else:\n","#                     aa = onehot_to_aa.get(tuple(int(x) for x in row))\n","#                     if aa:\n","#                         current.append(aa)\n","#                     else:\n","#                         raise ValueError(f\"Unknown one-hot vector: {row}\")\n","\n","#             if current:\n","#                 sequences.append((\"\".join(current), label))\n","\n","#             return sequences\n","\n","#         # === Load both files ===\n","#         positive_sequences = load_onehot_file(\"pos.txt\", label=1)\n","#         negative_sequences = load_onehot_file(\"neg.txt\", label=0)\n","\n","#         # === Combine and format as DataFrame\n","#         all_sequences = positive_sequences + negative_sequences\n","#         df = pd.DataFrame(all_sequences, columns=[\"sequence\", \"label\"])\n","#         df[\"id\"] = [f\"seq_{i}\" for i in range(len(df))]\n","#         df = df[[\"id\", \"sequence\", \"label\"]]\n","\n","#         # === Split into train/test (80/20 stratified)\n","#         train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n","\n","#         # === Save CSVs\n","#         train_df.to_csv(train_csv_path, index=False)\n","#         test_df.to_csv(test_csv_path, index=False)\n","\n","#         print(f\"✅ Saved training set to '{train_csv_path}'\")\n","#         print(f\"✅ Saved testing set to '{test_csv_path}'\")\n","# else:\n","#     print('AllergenAI dataset not chosen')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data from AlgPred 2.0"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9444,"status":"ok","timestamp":1743545518910,"user":{"displayName":"Jeff","userId":"15773939950998775573"},"user_tz":-120},"id":"ilcsg8tr0Nus","outputId":"6b11ab8a-9c74-4ba9-f7fb-5b59a4090162"},"outputs":[{"name":"stdout","output_type":"stream","text":["AlgPred 2.0 dataset chosen\n","⬇️  Downloading train_positive from https://webs.iiitd.edu.in/raghava/algpred2/datasets/train_positive.txt...\n","⬇️  Downloading train_negative from https://webs.iiitd.edu.in/raghava/algpred2/datasets/train_negative.txt...\n","⬇️  Downloading validation_positive from https://webs.iiitd.edu.in/raghava/algpred2/datasets/validation_positive.txt...\n","⬇️  Downloading validation_negative from https://webs.iiitd.edu.in/raghava/algpred2/datasets/validation_negative.txt...\n","✅ Saved training set to 'algpred2/algpred2_train.csv'\n","✅ Saved validation set to 'algpred2/algpred2_test.csv'\n"]}],"source":["if dataset_name in [\"algpred2\", \"algpred2_resplit\"]:\n","    print('AlgPred 2.0 dataset chosen')\n","    data_dir = dataset_name\n","    os.makedirs(data_dir, exist_ok=True)\n","\n","    train_csv_path = os.path.join(data_dir, f\"{dataset_name}_train.csv\")\n","    test_csv_path = os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","\n","    # Skip if both files already exist\n","    if os.path.exists(train_csv_path) and os.path.exists(test_csv_path):\n","        print(\"⏭️  Files already exist. Skipping download and parsing.\")\n","    else:\n","        # URLs from AlgPred 2.0\n","        datasets = {\n","            \"train_positive\": (\"https://webs.iiitd.edu.in/raghava/algpred2/datasets/train_positive.txt\", 1, \"train\"),\n","            \"train_negative\": (\"https://webs.iiitd.edu.in/raghava/algpred2/datasets/train_negative.txt\", 0, \"train\"),\n","            \"validation_positive\": (\"https://webs.iiitd.edu.in/raghava/algpred2/datasets/validation_positive.txt\", 1, \"val\"),\n","            \"validation_negative\": (\"https://webs.iiitd.edu.in/raghava/algpred2/datasets/validation_negative.txt\", 0, \"val\")\n","        }\n","\n","        def parse_fasta(fasta_text, label):\n","            sequences = []\n","            current_id = None\n","            current_seq = \"\"\n","            for line in fasta_text.strip().splitlines():\n","                line = line.strip()\n","                if line.startswith(\">\"):\n","                    if current_id is not None:\n","                        sequences.append((current_id, current_seq, label))\n","                    current_id = line[1:]\n","                    current_seq = \"\"\n","                else:\n","                    current_seq += line\n","            if current_id and current_seq:\n","                sequences.append((current_id, current_seq, label))\n","            return sequences\n","\n","        train_entries = []\n","        val_entries = []\n","\n","        for name, (url, label, split) in datasets.items():\n","            print(f\"⬇️  Downloading {name} from {url}...\")\n","            response = requests.get(url)\n","            entries = parse_fasta(response.text, label)\n","            if split == \"train\":\n","                train_entries.extend(entries)\n","            else:\n","                val_entries.extend(entries)\n","\n","        df_train = pd.DataFrame(train_entries, columns=[\"id\", \"sequence\", \"label\"])\n","        df_val = pd.DataFrame(val_entries, columns=[\"id\", \"sequence\", \"label\"])\n","\n","        df_train.to_csv(train_csv_path, index=False)\n","        df_val.to_csv(test_csv_path, index=False)\n","\n","        print(f\"✅ Saved training set to '{train_csv_path}'\")\n","        print(f\"✅ Saved validation set to '{test_csv_path}'\")\n","else:\n","    print('AlgPred 2.0 dataset not chosen')\n"]},{"cell_type":"markdown","metadata":{},"source":["# Embedding generation"]},{"cell_type":"markdown","metadata":{},"source":["## ESM-2 embedding extraction"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":206837,"status":"ok","timestamp":1743546270778,"user":{"displayName":"Jeff","userId":"15773939950998775573"},"user_tz":-120},"id":"1_Jv62ty0mrA","outputId":"778a1e22-2458-4988-a7f1-e5b0c49097ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["ESM-2 embedding extraction chosen\n","✅ ESM2 embedding files already exist in '/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/algpred2/'. Skipping embedding generation.\n"]}],"source":["if transformer_name == 'ESM-2_320dim':\n","    print('ESM-2 embedding extraction chosen')\n","    # === CONFIG ===\n","    feature_dim = 320           # ESM-2 T6-8M embedding size\n","    batch_size = 1              # Adjust based on memory\n","    data_dir = \"/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/\" + dataset_name     # All files live in a folder named after the dataset\n","\n","    # --- Ensure directory exists ---\n","    os.makedirs(data_dir, exist_ok=True)\n","\n","    # --- Construct dynamic file paths ---\n","    input_files = {\n","        \"train\": os.path.join(data_dir, f\"{dataset_name}_train.csv\"),\n","        \"test\": os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","    }\n","\n","    # --- Output file paths ---\n","    embedding_files = {\n","        \"train\": os.path.join(data_dir, f\"train_{dataset_name}_esm2_embeddings.csv\"),\n","        \"test\": os.path.join(data_dir, f\"test_{dataset_name}_esm2_embeddings.csv\")\n","    }\n","\n","    # Check if both embedding files exist\n","    if all(os.path.exists(f) for f in embedding_files.values()):\n","        print(f\"✅ ESM2 embedding files already exist in '{data_dir}/'. Skipping embedding generation.\")\n","    else:\n","        # --- Load ESM-2 model ---\n","        model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n","        batch_converter = alphabet.get_batch_converter()\n","        model.eval()\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model = model.to(device)\n","\n","        # --- Helper function ---\n","        def process_file(split_name, input_file):\n","            temp_file = os.path.join(data_dir, f\"{split_name}_{dataset_name}_esm2_embeddings_temp.csv\")\n","            final_file = os.path.join(data_dir, f\"{split_name}_{dataset_name}_esm2_embeddings.csv\")\n","\n","            # Load dataset\n","            df = pd.read_csv(input_file)\n","            sequences = list(df[\"sequence\"])\n","            labels = list(df[\"label\"])\n","            ids = list(df[\"id\"])\n","\n","            # Resume support\n","            if os.path.exists(temp_file):\n","                processed_ids = set(pd.read_csv(temp_file, usecols=[\"id\"])[\"id\"])\n","                print(f\"🔁 Resuming {split_name} from {temp_file} — {len(processed_ids)} entries already processed.\")\n","            else:\n","                processed_ids = set()\n","\n","            remaining_data = [(ids[i], sequences[i], labels[i]) for i in range(len(ids)) if ids[i] not in processed_ids]\n","\n","            # Output format\n","            fieldnames = [\"id\", \"label\"] + [f\"f{k}\" for k in range(feature_dim)]\n","            write_header = not os.path.exists(temp_file)\n","\n","            print(f\"⚙️  Extracting embeddings for {split_name} set... ({len(remaining_data)} sequences remaining)\")\n","\n","            with open(temp_file, mode=\"a\", newline=\"\") as f:\n","                writer = csv.DictWriter(f, fieldnames=fieldnames)\n","                if write_header:\n","                    writer.writeheader()\n","\n","                for i in tqdm(range(0, len(remaining_data), batch_size)):\n","                    batch = remaining_data[i:i + batch_size]\n","                    batch_ids = [x[0] for x in batch]\n","                    batch_seqs = [x[1] for x in batch]\n","                    batch_labels = [x[2] for x in batch]\n","\n","                    batch_data = [(batch_ids[j], batch_seqs[j]) for j in range(len(batch_seqs))]\n","                    _, _, batch_tokens = batch_converter(batch_data)\n","                    batch_tokens = batch_tokens.to(device)\n","\n","                    with torch.no_grad():\n","                        outputs = model(batch_tokens, repr_layers=[6])\n","                        token_representations = outputs[\"representations\"][6]\n","\n","                    rows = []\n","                    for j, (_, seq) in enumerate(batch_data):\n","                        representation = token_representations[j, 1:len(seq)+1].mean(0)\n","                        entry = {\n","                            \"id\": batch_ids[j],\n","                            \"label\": batch_labels[j],\n","                        }\n","                        for k in range(feature_dim):\n","                            entry[f\"f{k}\"] = representation[k].item()\n","                        rows.append(entry)\n","\n","                    writer.writerows(rows)\n","\n","            # Final save\n","            os.replace(temp_file, final_file)\n","            print(f\"✅ Final {split_name} embeddings saved to '{final_file}'\")\n","\n","        # --- Process each split ---\n","        for split, file in input_files.items():\n","            process_file(split, file)\n","else: print('ESM-2 embedding generation not chosen')"]},{"cell_type":"markdown","metadata":{},"source":["## ProtBert Embeddings "]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ProtBert embedding extraction chosen\n","✅ ProtBert embedding files already exist in '/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/algpred2/'. Skipping generation.\n"]}],"source":["if transformer_name == \"ProtBert_1024dim\":\n","    print('ProtBert embedding extraction chosen')\n","\n","    \n","\n","    # === CONFIG ===\n","    feature_dim = 1024\n","    batch_size = 1\n","    data_dir = \"/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/\" + dataset_name\n","    os.makedirs(data_dir, exist_ok=True)\n","\n","    input_files = {\n","        \"train\": os.path.join(data_dir, f\"{dataset_name}_train.csv\"),\n","        \"test\": os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","    }\n","\n","    embedding_files = {\n","        \"train\": os.path.join(data_dir, f\"train_{dataset_name}_protbert_embeddings.csv\"),\n","        \"test\": os.path.join(data_dir, f\"test_{dataset_name}_protbert_embeddings.csv\")\n","    }\n","\n","    if all(os.path.exists(f) for f in embedding_files.values()):\n","        print(f\"✅ ProtBert embedding files already exist in '{data_dir}/'. Skipping generation.\")\n","    else:\n","        # === Load ProtBert ===\n","        tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n","        model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n","        model.eval()\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model = model.to(device)\n","\n","        def process_file(split_name, input_file):\n","            temp_file = os.path.join(data_dir, f\"{split_name}_{dataset_name}_protbert_embeddings_temp.csv\")\n","            final_file = os.path.join(data_dir, f\"{split_name}_{dataset_name}_protbert_embeddings.csv\")\n","\n","            df = pd.read_csv(input_file)\n","            sequences = list(df[\"sequence\"])\n","            labels = list(df[\"label\"])\n","            ids = list(df[\"id\"])\n","\n","            if os.path.exists(temp_file):\n","                processed_ids = set(pd.read_csv(temp_file, usecols=[\"id\"])[\"id\"])\n","                print(f\"🔁 Resuming {split_name} from {temp_file} — {len(processed_ids)} already processed.\")\n","            else:\n","                processed_ids = set()\n","\n","            remaining_data = [(ids[i], sequences[i], labels[i]) for i in range(len(ids)) if ids[i] not in processed_ids]\n","\n","            fieldnames = [\"id\", \"label\"] + [f\"f{k}\" for k in range(feature_dim)]\n","            write_header = not os.path.exists(temp_file)\n","\n","            print(f\"⚙️  Extracting ProtBert embeddings for {split_name}... ({len(remaining_data)} sequences)\")\n","\n","            with open(temp_file, mode=\"a\", newline=\"\") as f:\n","                writer = csv.DictWriter(f, fieldnames=fieldnames)\n","                if write_header:\n","                    writer.writeheader()\n","\n","                for i in tqdm(range(0, len(remaining_data), batch_size)):\n","                    batch = remaining_data[i:i + batch_size]\n","                    batch_ids = [x[0] for x in batch]\n","                    batch_seqs = [x[1] for x in batch]\n","                    batch_labels = [x[2] for x in batch]\n","\n","                    # Preprocess for ProtBert\n","                    batch_seqs = [\" \".join(list(seq)) for seq in batch_seqs]\n","                    encoded_input = tokenizer(batch_seqs, return_tensors='pt', padding=True, truncation=True)\n","                    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n","\n","                    with torch.no_grad():\n","                        output = model(**encoded_input)\n","                        embeddings = output.last_hidden_state.mean(dim=1)\n","\n","                    rows = []\n","                    for j in range(len(batch)):\n","                        entry = {\n","                            \"id\": batch_ids[j],\n","                            \"label\": batch_labels[j],\n","                        }\n","                        for k in range(feature_dim):\n","                            entry[f\"f{k}\"] = embeddings[j][k].item()\n","                        rows.append(entry)\n","\n","                    writer.writerows(rows)\n","\n","            os.replace(temp_file, final_file)\n","            print(f\"✅ Final {split_name} embeddings saved to '{final_file}'\")\n","\n","        for split, file in input_files.items():\n","            process_file(split, file)\n","else:\n","    print('ProtBert embedding generation not chosen')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Prot T5 Embeddings"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ProtT5 embedding generation not chosen\n"]}],"source":["# if transformer_name == \"ProtT5_1024dim\":\n","#     print('ProtT5 embedding extraction chosen')\n","\n","#     # === CONFIG ===\n","#     feature_dim = 1024\n","#     batch_size = 1\n","#     data_dir = dataset_name\n","#     os.makedirs(data_dir, exist_ok=True)\n","\n","#     input_files = {\n","#         \"train\": os.path.join(data_dir, f\"{dataset_name}_train.csv\"),\n","#         \"test\": os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","#     }\n","\n","#     embedding_files = {\n","#         \"train\": os.path.join(data_dir, f\"train_{dataset_name}_{transformer_name}_embeddings.csv\"),\n","#         \"test\": os.path.join(data_dir, f\"test_{dataset_name}_{transformer_name}_embeddings.csv\")\n","#     }\n","\n","#     if all(os.path.exists(f) for f in embedding_files.values()):\n","#         print(f\"✅ ProtT5 embedding files already exist in '{data_dir}/'. Skipping generation.\")\n","#     else:\n","#         # === Load ProtT5 Encoder ===\n","#         from transformers import T5Tokenizer, T5EncoderModel\n","\n","#         tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", do_lower_case=False)\n","#         model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n","#         model.eval()\n","#         device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","#         model = model.to(device)\n","\n","#         def process_file(split_name, input_file):\n","#             temp_file = os.path.join(data_dir, f\"{split_name}_{dataset_name}_prott5_embeddings_temp.csv\")\n","#             final_file = os.path.join(data_dir, f\"{split_name}_{dataset_name}_prott5_embeddings.csv\")\n","\n","#             df = pd.read_csv(input_file)\n","#             sequences = list(df[\"sequence\"])\n","#             labels = list(df[\"label\"])\n","#             ids = list(df[\"id\"])\n","\n","#             if os.path.exists(temp_file):\n","#                 processed_ids = set(pd.read_csv(temp_file, usecols=[\"id\"])[\"id\"])\n","#                 print(f\"🔁 Resuming {split_name} from {temp_file} — {len(processed_ids)} already processed.\")\n","#             else:\n","#                 processed_ids = set()\n","\n","#             remaining_data = [(ids[i], sequences[i], labels[i]) for i in range(len(ids)) if ids[i] not in processed_ids]\n","\n","#             fieldnames = [\"id\", \"label\"] + [f\"f{k}\" for k in range(feature_dim)]\n","#             write_header = not os.path.exists(temp_file)\n","\n","#             print(f\"⚙️  Extracting ProtT5 embeddings for {split_name}... ({len(remaining_data)} sequences)\")\n","\n","#             with open(temp_file, mode=\"a\", newline=\"\") as f:\n","#                 writer = csv.DictWriter(f, fieldnames=fieldnames)\n","#                 if write_header:\n","#                     writer.writeheader()\n","\n","#                 for i in tqdm(range(0, len(remaining_data), batch_size)):\n","#                     batch = remaining_data[i:i + batch_size]\n","#                     batch_ids = [x[0] for x in batch]\n","#                     batch_seqs = [x[1] for x in batch]\n","#                     batch_labels = [x[2] for x in batch]\n","\n","#                     # Preprocess for ProtT5\n","#                     batch_seqs = [\" \".join(list(seq)) for seq in batch_seqs]\n","#                     encoded_input = tokenizer(batch_seqs, return_tensors='pt', padding=True, truncation=True)\n","#                     encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n","\n","#                     with torch.no_grad():\n","#                         output = model(**encoded_input)\n","#                         embeddings = output.last_hidden_state.mean(dim=1)\n","\n","#                     rows = []\n","#                     for j in range(len(batch)):\n","#                         entry = {\n","#                             \"id\": batch_ids[j],\n","#                             \"label\": batch_labels[j],\n","#                         }\n","#                         for k in range(feature_dim):\n","#                             entry[f\"f{k}\"] = embeddings[j][k].item()\n","#                         rows.append(entry)\n","\n","#                     writer.writerows(rows)\n","\n","#             os.replace(temp_file, final_file)\n","#             print(f\"✅ Final {split_name} embeddings saved to '{final_file}'\")\n","\n","#         for split, file in input_files.items():\n","#             process_file(split, file)\n","# else:\n","#     print('ProtT5 embedding generation not chosen')\n"]},{"cell_type":"markdown","metadata":{},"source":["# concatanate ESM fold data with 3d DSSP data into vector"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["✅ Merged train set saved to: /Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/algpred2/train_merged_with_dssp_protbert_embeddings.csv\n","Train set shape after merge: (15311, 1040)\n","✅ Merged test set saved to: /Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/algpred2/test_merged_with_dssp_protbert_embeddings.csv\n","Test set shape after merge: (3946, 1040)\n"]}],"source":["import pandas as pd\n","\n","# === TRAIN SET ===\n","# Load original training data and DSSP features\n","df_train_original = pd.read_csv('/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/algpred2/train_algpred2_protbert_embeddings.csv')\n","df_train_dssp = pd.read_csv('/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/dssp_train_features.csv')\n","\n","# Merge on matching protein IDs\n","df_train_merged = pd.merge(df_train_original, df_train_dssp, left_on='id', right_on='protein', how='inner')\n","df_train_merged = df_train_merged.drop(columns=['protein'])\n","\n","# Save merged training data\n","train_output_path = '/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/algpred2/train_merged_with_dssp_protbert_embeddings.csv'\n","df_train_merged.to_csv(train_output_path, index=False)\n","print(f\"✅ Merged train set saved to: {train_output_path}\")\n","print(\"Train set shape after merge:\", df_train_merged.shape)\n","\n","\n","# === TEST SET ===\n","# Load original test data and DSSP features\n","df_test_original = pd.read_csv('/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/algpred2/test_algpred2_protbert_embeddings.csv')\n","df_test_dssp = pd.read_csv('/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/dssp_test_features.csv')\n","\n","# Merge on matching protein IDs\n","df_test_merged = pd.merge(df_test_original, df_test_dssp, left_on='id', right_on='protein', how='inner')\n","df_test_merged = df_test_merged.drop(columns=['protein'])\n","\n","# Save merged test data\n","test_output_path = '/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/algpred2/test_merged_with_dssp_protbert_embeddings.csv'\n","df_test_merged.to_csv(test_output_path, index=False)\n","print(f\"✅ Merged test set saved to: {test_output_path}\")\n","print(\"Test set shape after merge:\", df_test_merged.shape)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Models"]},{"cell_type":"markdown","metadata":{},"source":["## Ridge Regression"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Ridge Classifier model chosen\n","algpred2 dataset chosen\n","✅ Loaded: Train=(16120, 320), Test=(4030, 320)\n","\n","📉 DummyClassifier (Stratified) on Training Set (CV):\n","\n","📊 Dummy ROC-AUC: 0.4991 ± 0.0000\n","\n","🚀 5-Fold Cross-Validation (RidgeClassifier) on Training Set...\n","\n","\n","🚀 5-Fold Cross-Validation (RidgeClassifier) on Training Set...\n","\n","📂 Fold 1 Metrics:\n"," - Accuracy     : 0.9395\n"," - Sensitivity  : 0.9479\n"," - Specificity  : 0.9311\n"," - ROC-AUC      : 0.9395\n"," - MCC          : 0.8792\n","------\n","📂 Fold 2 Metrics:\n"," - Accuracy     : 0.9336\n"," - Sensitivity  : 0.9367\n"," - Specificity  : 0.9305\n"," - ROC-AUC      : 0.9336\n"," - MCC          : 0.8673\n","------\n","📂 Fold 3 Metrics:\n"," - Accuracy     : 0.9423\n"," - Sensitivity  : 0.9584\n"," - Specificity  : 0.9262\n"," - ROC-AUC      : 0.9423\n"," - MCC          : 0.8851\n","------\n","📂 Fold 4 Metrics:\n"," - Accuracy     : 0.9414\n"," - Sensitivity  : 0.9535\n"," - Specificity  : 0.9293\n"," - ROC-AUC      : 0.9414\n"," - MCC          : 0.8830\n","------\n","📂 Fold 5 Metrics:\n"," - Accuracy     : 0.9361\n"," - Sensitivity  : 0.9498\n"," - Specificity  : 0.9225\n"," - ROC-AUC      : 0.9361\n"," - MCC          : 0.8725\n","------\n","\n","✅ Mean CV ROC-AUC: 0.9386 ± 0.0037 (SE = 0.0016)\n","\n","🔒 Final Evaluation on Hold-Out Test Set...\n","\n","🧪 Final Test Set Metrics:\n"," - Accuracy     : 0.8583\n"," - Sensitivity  : 0.7841\n"," - Specificity  : 0.9325\n"," - ROC-AUC      : 0.8583\n"," - MCC          : 0.7246\n","\n","🧾 Confusion Matrix on Hold-Out Test Set:\n","\n","[[1879  136]\n"," [ 435 1580]]\n","\n","🧪 Y-Scrambling (sanity check) on Training Set...\n","\n","🔀 Y-Scrambled ROC-AUC: 0.4958 ± 0.0047\n","👉 This should be near 0.5 if your real model learned something.\n"]}],"source":["model_name = \"Ridge\"\n","if model_name == 'Ridge':\n","    log_filename = f\"{model_name}_{transformer_name}_{dataset_name}_dssp_output.txt\"\n","    log_path = os.path.join(\"/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/logs\", log_filename)\n","    os.makedirs(\"/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/logs\", exist_ok=True)\n","\n","    class Tee:\n","        def __init__(self, *streams):\n","            self.streams = streams\n","        def write(self, text):\n","            for stream in self.streams:\n","                stream.write(text)\n","                stream.flush()\n","        def flush(self):\n","            for stream in self.streams:\n","                stream.flush()\n","\n","    with open(log_path, \"w\") as log_file:\n","        tee = Tee(sys.stdout, log_file)\n","        with redirect_stdout(tee), redirect_stderr(tee):\n","\n","            print('Ridge Classifier model chosen')\n","            print(dataset_name, 'dataset chosen')\n","\n","            data_dir = \"/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/\" + dataset_name\n","            embedding_files = {\n","                \"train\": os.path.join(data_dir, f\"train_merged_with_dssp_ESM-2_320dim.csv\"),#train_{dataset_name}_esm2_embeddings.csv\n","                \"test\": os.path.join(data_dir, f\"test_merged_with_dssp_ESM-2_320dim.csv\")#test_{dataset_name}_esm2_embeddings.csv\n","            }\n","\n","            df_train = pd.read_csv(embedding_files[\"train\"])\n","            df_test = pd.read_csv(embedding_files[\"test\"])\n","\n","            feature_cols = [f\"f{i}\" for i in range(320)]\n","            X_train_full = df_train[feature_cols].values\n","            y_train_full = df_train[\"label\"].values\n","            X_test = df_test[feature_cols].values\n","            y_test = df_test[\"label\"].values\n","\n","            print(f\"✅ Loaded: Train={X_train_full.shape}, Test={X_test.shape}\")\n","\n","            print(\"\\n📉 DummyClassifier (Stratified) on Training Set (CV):\\n\")\n","            dummy = DummyClassifier(strategy=\"stratified\", random_state=42)\n","            dummy_aucs = []\n","            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","            for train_idx, val_idx in cv.split(X_train_full, y_train_full):\n","                dummy.fit(X_train_full[train_idx], y_train_full[train_idx])\n","                y_dummy_proba = dummy.predict_proba(X_train_full[val_idx])[:, 1]\n","                auc = roc_auc_score(y_train_full[val_idx], y_dummy_proba)\n","                dummy_aucs.append(auc)\n","\n","            print(f\"📊 Dummy ROC-AUC: {np.mean(dummy_aucs):.4f} ± {np.std(dummy_aucs):.4f}\")\n","\n","            print(\"\\n🚀 5-Fold Cross-Validation (RidgeClassifier) on Training Set...\\n\")\n","            from sklearn.metrics import (\n","                roc_auc_score, accuracy_score, recall_score, confusion_matrix, matthews_corrcoef\n","            )\n","\n","            def evaluate_model(y_true, y_pred):\n","                acc = accuracy_score(y_true, y_pred)\n","                sens = recall_score(y_true, y_pred, pos_label=1)\n","                spec = recall_score(y_true, y_pred, pos_label=0)\n","                auc = roc_auc_score(y_true, y_pred)\n","                mcc = matthews_corrcoef(y_true, y_pred)\n","                return acc, sens, spec, auc, mcc\n","\n","            # 5-Fold Cross-Validation with RidgeClassifier\n","            print(\"\\n🚀 5-Fold Cross-Validation (RidgeClassifier) on Training Set...\\n\")\n","            ridge_aucs = []\n","            for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_full, y_train_full)):\n","                X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]\n","                y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n","\n","                clf = RidgeClassifier()\n","                clf.fit(X_train, y_train)\n","\n","                y_pred = clf.predict(X_val)\n","                acc, sens, spec, auc, mcc = evaluate_model(y_val, y_pred)\n","                ridge_aucs.append(auc)\n","\n","                print(f\"📂 Fold {fold+1} Metrics:\")\n","                print(f\" - Accuracy     : {acc:.4f}\")\n","                print(f\" - Sensitivity  : {sens:.4f}\")\n","                print(f\" - Specificity  : {spec:.4f}\")\n","                print(f\" - ROC-AUC      : {auc:.4f}\")\n","                print(f\" - MCC          : {mcc:.4f}\")\n","                print(\"------\")\n","\n","            mean_auc = np.mean(ridge_aucs)\n","            std_auc = np.std(ridge_aucs, ddof=1)\n","            se_auc = std_auc / np.sqrt(len(ridge_aucs))\n","            print(f\"\\n✅ Mean CV ROC-AUC: {mean_auc:.4f} ± {std_auc:.4f} (SE = {se_auc:.4f})\")\n","\n","            # Final Evaluation on Test Set\n","            print(\"\\n🔒 Final Evaluation on Hold-Out Test Set...\\n\")\n","            clf_final = RidgeClassifier()\n","            clf_final.fit(X_train_full, y_train_full)\n","\n","            y_test_pred = clf_final.predict(X_test)\n","            acc, sens, spec, test_auc, mcc = evaluate_model(y_test, y_test_pred)\n","\n","            print(\"🧪 Final Test Set Metrics:\")\n","            print(f\" - Accuracy     : {acc:.4f}\")\n","            print(f\" - Sensitivity  : {sens:.4f}\")\n","            print(f\" - Specificity  : {spec:.4f}\")\n","            print(f\" - ROC-AUC      : {test_auc:.4f}\")\n","            print(f\" - MCC          : {mcc:.4f}\")\n","\n","            print(\"\\n🧾 Confusion Matrix on Hold-Out Test Set:\\n\")\n","            cm = confusion_matrix(y_test, y_test_pred)\n","            print(cm)\n","\n","\n","            print(\"\\n🧪 Y-Scrambling (sanity check) on Training Set...\\n\")\n","            y_scrambled = y_train_full.copy()\n","            random.seed(42)\n","            random.shuffle(y_scrambled)\n","\n","            scrambled_aucs = []\n","            for train_idx, val_idx in cv.split(X_train_full, y_scrambled):\n","                X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]\n","                y_train, y_val = y_scrambled[train_idx], y_scrambled[val_idx]\n","\n","                clf_scrambled = RidgeClassifier()\n","                clf_scrambled.fit(X_train, y_train)\n","                y_pred_scrambled = clf_scrambled.predict(X_val)\n","                auc = roc_auc_score(y_val, y_pred_scrambled)\n","                scrambled_aucs.append(auc)\n","\n","            print(f\"🔀 Y-Scrambled ROC-AUC: {np.mean(scrambled_aucs):.4f} ± {np.std(scrambled_aucs):.4f}\")\n","            print(\"👉 This should be near 0.5 if your real model learned something.\")\n","else: print('Ridge model not chosen')"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Precision: 0.9207\n","Recall (Sensitivity): 0.7841\n","F1 Score: 0.8470\n"]}],"source":["# Confusion matrix values\n","TP = 1580  # True Positives\n","FP = 136    # False Positives\n","FN = 435   # False Negatives\n","TN = 1879  # True Negatives (not used for F1, but included for completeness)\n","\n","# Calculate Precision and Recall\n","precision = TP / (TP + FP)\n","recall = TP / (TP + FN)\n","\n","# Calculate F1 Score\n","f1_score = 2 * (precision * recall) / (precision + recall)\n","\n","# Print results\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall (Sensitivity): {recall:.4f}\")\n","print(f\"F1 Score: {f1_score:.4f}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## XGBoosted"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":101704,"status":"ok","timestamp":1743547169212,"user":{"displayName":"Jeff","userId":"15773939950998775573"},"user_tz":-120},"id":"dvQ6gYSmeMsx","outputId":"9e230e2d-17e6-48b8-c75a-c17026d7c36f"},"outputs":[{"name":"stdout","output_type":"stream","text":["XGBoost model chosen\n","algpred2 dataset chosen\n","✅ Loaded: Train=(15311, 1038), Test=(3946, 1038)\n","\n","📉 DummyClassifier (Stratified) on Training Set (CV):\n","\n","📊 Dummy ROC-AUC: 0.4936 ± 0.0001\n","\n","🔍 Hyperparameter Tuning with GridSearchCV...\n","\n","Fitting 5 folds for each of 72 candidates, totalling 360 fits\n","/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n","  warnings.warn(\n","\n","🏆 Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 100, 'subsample': 0.8}\n","🏆 Best CV ROC-AUC: 0.9985\n","\n","🔒 Final Evaluation on Hold-Out Test Set...\n","\n","✅ Model saved to: XGBoost_ProtBert_1024dim_algpred2_xgboost_model.pkl\n","🧪 Final Test Set Metrics:\n"," - Accuracy     : 0.8160\n"," - Sensitivity  : 0.6526\n"," - Specificity  : 0.9860\n"," - ROC-AUC      : 0.9548\n"," - MCC          : 0.6741\n"," - F1-Score     : 0.7834\n"," - Precision    : 0.9799\n","\n","🧾 Confusion Matrix on Hold-Out Test Set:\n","\n","          Predicted 0  Predicted 1\n","Actual 0         1907           27\n","Actual 1          699         1313\n","\n","🧪 Y-Scrambling (sanity check) on Training Set...\n","\n","🔀 Y-Scrambled ROC-AUC: 0.5030 ± 0.0074\n","👉 This should be near 0.5 if your real model learned something.\n"]}],"source":["import os\n","import sys\n","import numpy as np\n","import pandas as pd\n","from contextlib import redirect_stdout, redirect_stderr\n","from sklearn.model_selection import StratifiedKFold, GridSearchCV\n","from sklearn.metrics import (\n","    roc_auc_score, accuracy_score, recall_score,\n","    confusion_matrix, matthews_corrcoef, f1_score, precision_score\n",")\n","from sklearn.dummy import DummyClassifier\n","import joblib\n","import xgboost as xgb\n","import random\n","\n","# --- Parameters for grid search (edit as needed) ---\n","param_grid = {\n","    'n_estimators': [50, 100],\n","    'max_depth': [3, 5, 7],\n","    'learning_rate': [0.01, 0.1, 0.2],\n","    'subsample': [0.8, 1.0],\n","    'colsample_bytree': [0.8, 1.0]\n","}\n","\n","model_name = \"XGBoost\"\n","if model_name == 'XGBoost':\n","    # === Create log file path ===\n","    log_filename = f\"{model_name}_{transformer_name}_{dataset_name}_dssp_output.txt\"\n","    log_path = os.path.join(\"/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/logs\", log_filename)\n","    os.makedirs(\"/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/logs\", exist_ok=True)\n","\n","    class Tee:\n","        def __init__(self, *streams):\n","            self.streams = streams\n","        def write(self, text):\n","            for stream in self.streams:\n","                stream.write(text)\n","                stream.flush()\n","        def flush(self):\n","            for stream in self.streams:\n","                stream.flush()\n","\n","    def evaluate_model(y_true, y_pred, y_proba):\n","        acc = accuracy_score(y_true, y_pred)\n","        sens = recall_score(y_true, y_pred, pos_label=1)\n","        spec = recall_score(y_true, y_pred, pos_label=0)\n","        auc = roc_auc_score(y_true, y_proba)\n","        mcc = matthews_corrcoef(y_true, y_pred)\n","        f1 = f1_score(y_true, y_pred)\n","        prec = precision_score(y_true, y_pred)\n","        return acc, sens, spec, auc, mcc, f1, prec\n","\n","    with open(log_path, \"w\") as log_file:\n","        tee = Tee(sys.stdout, log_file)\n","        with redirect_stdout(tee), redirect_stderr(tee):\n","\n","            print('XGBoost model chosen')\n","            print(dataset_name, 'dataset chosen')\n","\n","            # Step 1: Load Data\n","            data_dir = \"/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/\" + dataset_name\n","            embedding_files = {\n","                \"train\": os.path.join(data_dir, f\"train_merged_with_dssp_protbert_embeddings.csv\"),#train_{dataset_name}_esm2_embeddings.csv\n","                \"test\": os.path.join(data_dir, f\"test_merged_with_dssp_protbert_embeddings.csv\")#test_{dataset_name}_esm2_embeddings.csv\n","            }\n","            df_train = pd.read_csv(embedding_files[\"train\"])\n","            df_test = pd.read_csv(embedding_files[\"test\"])\n","\n","            # All columns from index 2 onward are features\n","            X_train_full = df_train.iloc[:, 2:].values\n","            y_train_full = df_train[\"label\"].values\n","            X_test = df_test.iloc[:, 2:].values\n","            y_test = df_test[\"label\"].values\n","\n","            print(f\"✅ Loaded: Train={X_train_full.shape}, Test={X_test.shape}\")\n","\n","            # Step 2: DummyClassifier\n","            print(\"\\n📉 DummyClassifier (Stratified) on Training Set (CV):\\n\")\n","            dummy = DummyClassifier(strategy=\"stratified\", random_state=42)\n","            dummy_aucs = []\n","            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","            for train_idx, val_idx in cv.split(X_train_full, y_train_full):\n","                dummy.fit(X_train_full[train_idx], y_train_full[train_idx])\n","                y_dummy_proba = dummy.predict_proba(X_train_full[val_idx])[:, 1]\n","                auc = roc_auc_score(y_train_full[val_idx], y_dummy_proba)\n","                dummy_aucs.append(auc)\n","            print(f\"📊 Dummy ROC-AUC: {np.mean(dummy_aucs):.4f} ± {np.std(dummy_aucs):.4f}\")\n","\n","            # Step 3: Hyperparameter tuning with GridSearchCV\n","            print(\"\\n🔍 Hyperparameter Tuning with GridSearchCV...\\n\")\n","            base_clf = xgb.XGBClassifier(eval_metric=\"logloss\", random_state=42)\n","            grid_search = GridSearchCV(\n","                estimator=base_clf,\n","                param_grid=param_grid,\n","                scoring='roc_auc',\n","                cv=cv,\n","                n_jobs=-1,\n","                verbose=1\n","            )\n","            grid_search.fit(X_train_full, y_train_full)\n","            print(f\"\\n🏆 Best Parameters: {grid_search.best_params_}\")\n","            print(f\"🏆 Best CV ROC-AUC: {grid_search.best_score_:.4f}\")\n","\n","            # Step 4: Final Test\n","            print(\"\\n🔒 Final Evaluation on Hold-Out Test Set...\\n\")\n","            clf_final = xgb.XGBClassifier(**grid_search.best_params_, eval_metric=\"logloss\", random_state=42)\n","            clf_final.fit(X_train_full, y_train_full)\n","\n","            # Save the trained XGBoost model\n","            model_path = f\"{model_name}_{transformer_name}_{dataset_name}_xgboost_model.pkl\"\n","            joblib.dump(clf_final, model_path)\n","            print(f\"✅ Model saved to: {model_path}\")\n","\n","            y_test_pred = clf_final.predict(X_test)\n","            y_test_proba = clf_final.predict_proba(X_test)[:, 1]\n","\n","            acc, sens, spec, test_auc, mcc, f1, prec = evaluate_model(y_test, y_test_pred, y_test_proba)\n","            print(\"🧪 Final Test Set Metrics:\")\n","            print(f\" - Accuracy     : {acc:.4f}\")\n","            print(f\" - Sensitivity  : {sens:.4f}\")\n","            print(f\" - Specificity  : {spec:.4f}\")\n","            print(f\" - ROC-AUC      : {test_auc:.4f}\")\n","            print(f\" - MCC          : {mcc:.4f}\")\n","            print(f\" - F1-Score     : {f1:.4f}\")\n","            print(f\" - Precision    : {prec:.4f}\")\n","\n","            print(\"\\n🧾 Confusion Matrix on Hold-Out Test Set:\\n\")\n","            cm = confusion_matrix(y_test, y_test_pred)\n","            cm_df = pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n","            print(cm_df)\n","\n","            # Step 5: Y-Scrambling (sanity check)\n","            print(\"\\n🧪 Y-Scrambling (sanity check) on Training Set...\\n\")\n","            y_scrambled = y_train_full.copy()\n","            random.seed(42)\n","            random.shuffle(y_scrambled)\n","            scrambled_aucs = []\n","            for train_idx, val_idx in cv.split(X_train_full, y_scrambled):\n","                X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]\n","                y_train, y_val = y_scrambled[train_idx], y_scrambled[val_idx]\n","\n","                clf_scrambled = xgb.XGBClassifier(**grid_search.best_params_, eval_metric=\"logloss\", random_state=42)\n","                clf_scrambled.fit(X_train, y_train)\n","                y_proba_scrambled = clf_scrambled.predict_proba(X_val)[:, 1]\n","                auc = roc_auc_score(y_val, y_proba_scrambled)\n","                scrambled_aucs.append(auc)\n","            print(f\"🔀 Y-Scrambled ROC-AUC: {np.mean(scrambled_aucs):.4f} ± {np.std(scrambled_aucs):.4f}\")\n","            print(\"👉 This should be near 0.5 if your real model learned something.\")\n","else:\n","    print('XGBoosted model not chosen')\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Precision: 0.9713\n","Recall (Sensitivity): 0.6541\n","F1 Score: 0.7817\n"]}],"source":["# Confusion matrix values\n","TP = 1318  # True Positives\n","FP = 39    # False Positives\n","FN = 697   # False Negatives\n","TN = 1976  # True Negatives (not used for F1, but included for completeness)\n","\n","# Calculate Precision and Recall\n","precision = TP / (TP + FP)\n","recall = TP / (TP + FN)\n","\n","# Calculate F1 Score\n","f1_score = 2 * (precision * recall) / (precision + recall)\n","\n","# Print results\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall (Sensitivity): {recall:.4f}\")\n","print(f\"F1 Score: {f1_score:.4f}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## FeedForwardNeuralNetwork"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["FFNN model chosen\n","algpred2 dataset chosen\n","✅ Loaded: Train=(16120, 320), Test=(4030, 320)\n","\n","📉 DummyClassifier (Stratified) on Training Set (CV):\n","\n","📊 Dummy ROC-AUC: 0.4991 ± 0.0000\n","\n","🚀 5-Fold Cross-Validation (NN) on Training Set...\n","\n","📂 Fold 1 Metrics:\n"," - Accuracy     : 0.9696\n"," - Sensitivity  : 0.9677\n"," - Specificity  : 0.9715\n"," - ROC-AUC      : 0.9907\n"," - MCC          : 0.9392\n","------\n","📂 Fold 2 Metrics:\n"," - Accuracy     : 0.9708\n"," - Sensitivity  : 0.9733\n"," - Specificity  : 0.9684\n"," - ROC-AUC      : 0.9936\n"," - MCC          : 0.9417\n","------\n","📂 Fold 3 Metrics:\n"," - Accuracy     : 0.9640\n"," - Sensitivity  : 0.9826\n"," - Specificity  : 0.9454\n"," - ROC-AUC      : 0.9944\n"," - MCC          : 0.9287\n","------\n","📂 Fold 4 Metrics:\n"," - Accuracy     : 0.9665\n"," - Sensitivity  : 0.9684\n"," - Specificity  : 0.9646\n"," - ROC-AUC      : 0.9941\n"," - MCC          : 0.9330\n","------\n","📂 Fold 5 Metrics:\n"," - Accuracy     : 0.9690\n"," - Sensitivity  : 0.9684\n"," - Specificity  : 0.9696\n"," - ROC-AUC      : 0.9952\n"," - MCC          : 0.9380\n","------\n","\n","✅ Mean CV ROC-AUC: 0.9936 ± 0.0015\n","\n","🔒 Final Evaluation on Hold-Out Test Set...\n","\n","🧪 Final Test Set Metrics:\n"," - Accuracy     : 0.8318\n"," - Sensitivity  : 0.6913\n"," - Specificity  : 0.9722\n"," - ROC-AUC      : 0.9172\n"," - MCC          : 0.6914\n","\n","🧾 Confusion Matrix on Hold-Out Test Set:\n","\n","          Predicted 0  Predicted 1\n","Actual 0         1959           56\n","Actual 1          622         1393\n"]}],"source":["model_name = \"FFNN\"\n","if model_name == 'FFNN':\n","    # Define NN model\n","    class ProteinMLP(nn.Module):\n","        def __init__(self):\n","            super().__init__()\n","            self.net = nn.Sequential(\n","                nn.Linear(320, 256),\n","                nn.ReLU(),\n","                nn.Dropout(0.3),\n","                nn.Linear(256, 128),\n","                nn.ReLU(),\n","                nn.Dropout(0.3),\n","                nn.Linear(128, 1),\n","                nn.Sigmoid()\n","            )\n","\n","        def forward(self, x):\n","            return self.net(x)\n","    # === Create log file path ===\n","    log_filename = f\"{model_name}_{transformer_name}_{dataset_name}_output.txt\"\n","    log_path = os.path.join(\"/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/logs\", log_filename)\n","    os.makedirs(\"/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/logs\", exist_ok=True)\n","    from sklearn.metrics import (\n","    roc_auc_score, accuracy_score, recall_score, confusion_matrix, matthews_corrcoef\n",")\n","\n","    def evaluate_model(y_true, y_pred, y_proba):\n","        acc = accuracy_score(y_true, y_pred)\n","        sens = recall_score(y_true, y_pred, pos_label=1)\n","        spec = recall_score(y_true, y_pred, pos_label=0)\n","        auc = roc_auc_score(y_true, y_proba)\n","        mcc = matthews_corrcoef(y_true, y_pred)\n","        return acc, sens, spec, auc, mcc\n","\n","    class Tee:\n","        def __init__(self, *streams):\n","            self.streams = streams\n","\n","        def write(self, text):\n","            for stream in self.streams:\n","                stream.write(text)\n","                stream.flush()\n","\n","        def flush(self):\n","            for stream in self.streams:\n","                stream.flush()\n","\n","    with open(log_path, \"w\") as log_file:\n","        tee = Tee(sys.stdout, log_file)\n","        with redirect_stdout(tee), redirect_stderr(tee):\n","\n","            print('FFNN model chosen')\n","   \n","            print(dataset_name, 'dataset chosen')\n","   \n","\n","            # ====================================\n","            # Step 1: Load Data\n","            # ====================================\n","            data_dir = \"/Users/rikardpettersson/Library/Mobile Documents/com~apple~CloudDocs/Documents/ETH Chemistry Ms/Digital Chemistry/\" + dataset_name\n","            embedding_files = {\n","                \"train\": os.path.join(data_dir, f\"train_{dataset_name}_esm2_embeddings.csv\"),\n","                \"test\": os.path.join(data_dir, f\"test_{dataset_name}_esm2_embeddings.csv\")\n","            }\n","\n","            df_train = pd.read_csv(embedding_files[\"train\"])\n","            df_test = pd.read_csv(embedding_files[\"test\"])\n","\n","            feature_cols = [f\"f{i}\" for i in range(320)]\n","            X_train_full = df_train[feature_cols].values.astype(np.float32)\n","            y_train_full = df_train[\"label\"].values.astype(np.float32)\n","            X_test = df_test[feature_cols].values.astype(np.float32)\n","            y_test = df_test[\"label\"].values.astype(np.float32)\n","\n","            print(f\"✅ Loaded: Train={X_train_full.shape}, Test={X_test.shape}\")\n","\n","            # ====================================\n","            # Step 2: DummyClassifier Baseline\n","            # ====================================\n","            print(\"\\n📉 DummyClassifier (Stratified) on Training Set (CV):\\n\")\n","            dummy = DummyClassifier(strategy=\"stratified\", random_state=42)\n","            dummy_aucs = []\n","            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","            for train_idx, val_idx in cv.split(X_train_full, y_train_full):\n","                dummy.fit(X_train_full[train_idx], y_train_full[train_idx])\n","                y_val = y_train_full[val_idx]\n","                y_dummy_proba = dummy.predict_proba(X_train_full[val_idx])\n","                if y_dummy_proba.shape[1] == 2:\n","                    y_dummy_proba = y_dummy_proba[:, 1]\n","                else:\n","                    y_dummy_proba = np.zeros_like(y_val)\n","                auc = roc_auc_score(y_val, y_dummy_proba)\n","                dummy_aucs.append(auc)\n","\n","            print(f\"📊 Dummy ROC-AUC: {np.mean(dummy_aucs):.4f} ± {np.std(dummy_aucs):.4f}\")\n","\n","            # ====================================\n","            # Step 3: 5-Fold CV with NN\n","            # ====================================\n","            print(\"\\n🚀 5-Fold Cross-Validation (NN) on Training Set...\\n\")\n","            nn_aucs = []\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","            for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_full, y_train_full)):\n","                X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]\n","                y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n","\n","                train_ds = TensorDataset(torch.tensor(X_train), torch.tensor(y_train).unsqueeze(1))\n","                val_ds = TensorDataset(torch.tensor(X_val), torch.tensor(y_val).unsqueeze(1))\n","                train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n","                val_loader = DataLoader(val_ds, batch_size=64)\n","\n","                model = ProteinMLP().to(device)\n","                criterion = nn.BCELoss()\n","                optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","                # Training loop\n","                model.train()\n","                for epoch in range(10):\n","                    for xb, yb in train_loader:\n","                        xb, yb = xb.to(device), yb.to(device)\n","                        optimizer.zero_grad()\n","                        preds = model(xb)\n","                        loss = criterion(preds, yb)\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # Validation\n","                model.eval()\n","                all_preds = []\n","                with torch.no_grad():\n","                    for xb, _ in val_loader:\n","                        xb = xb.to(device)\n","                        preds = model(xb).cpu().numpy()\n","                        all_preds.extend(preds)\n","                preds_bin = (np.array(all_preds) > 0.5).astype(int)\n","\n","                all_preds_array = np.array(all_preds).reshape(-1)\n","                preds_bin = (all_preds_array > 0.5).astype(int)\n","\n","                acc, sens, spec, auc, mcc = evaluate_model(y_val, preds_bin, all_preds_array)\n","                nn_aucs.append(auc)\n","\n","                print(f\"📂 Fold {fold+1} Metrics:\")\n","                print(f\" - Accuracy     : {acc:.4f}\")\n","                print(f\" - Sensitivity  : {sens:.4f}\")\n","                print(f\" - Specificity  : {spec:.4f}\")\n","                print(f\" - ROC-AUC      : {auc:.4f}\")\n","                print(f\" - MCC          : {mcc:.4f}\")\n","                print(\"------\")\n","\n","            print(f\"\\n✅ Mean CV ROC-AUC: {np.mean(nn_aucs):.4f} ± {np.std(nn_aucs):.4f}\")\n","\n","            # ====================================\n","            # Step 4: Final Test Set Evaluation\n","            # ====================================\n","            print(\"\\n🔒 Final Evaluation on Hold-Out Test Set...\\n\")\n","            train_ds = TensorDataset(torch.tensor(X_train_full), torch.tensor(y_train_full).unsqueeze(1))\n","            test_ds = TensorDataset(torch.tensor(X_test), torch.tensor(y_test).unsqueeze(1))\n","            train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n","            test_loader = DataLoader(test_ds, batch_size=64)\n","\n","            final_model = ProteinMLP().to(device)\n","            optimizer = optim.Adam(final_model.parameters(), lr=1e-3)\n","            criterion = nn.BCELoss()\n","\n","            final_model.train()\n","            for epoch in range(20):\n","                for xb, yb in train_loader:\n","                    xb, yb = xb.to(device), yb.to(device)\n","                    optimizer.zero_grad()\n","                    preds = final_model(xb)\n","                    loss = criterion(preds, yb)\n","                    loss.backward()\n","                    optimizer.step()\n","\n","            final_model.eval()\n","            y_test_preds, y_test_probs = [], []\n","            with torch.no_grad():\n","                for xb, _ in test_loader:\n","                    xb = xb.to(device)\n","                    probs = final_model(xb).cpu().numpy()\n","                    preds = (probs > 0.5).astype(int)\n","                    y_test_preds.extend(preds)\n","                    y_test_probs.extend(probs)\n","\n","            y_test_probs_array = np.array(y_test_probs).reshape(-1)\n","            y_test_preds_array = np.array(y_test_preds).reshape(-1)\n","\n","            acc, sens, spec, test_auc, mcc = evaluate_model(y_test, y_test_preds_array, y_test_probs_array)\n","\n","            print(\"🧪 Final Test Set Metrics:\")\n","            print(f\" - Accuracy     : {acc:.4f}\")\n","            print(f\" - Sensitivity  : {sens:.4f}\")\n","            print(f\" - Specificity  : {spec:.4f}\")\n","            print(f\" - ROC-AUC      : {test_auc:.4f}\")\n","            print(f\" - MCC          : {mcc:.4f}\")\n","\n","            print(\"\\n🧾 Confusion Matrix on Hold-Out Test Set:\\n\")\n","            cm = confusion_matrix(y_test, y_test_preds_array)\n","            cm_df = pd.DataFrame(cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n","            print(cm_df)\n","\n","else: print('FFNN model not chosen')"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Precision: 0.9614\n","Recall (Sensitivity): 0.6913\n","F1 Score: 0.8043\n"]}],"source":["# Confusion matrix values\n","TP = 1393  # True Positives\n","FP = 56    # False Positives\n","FN = 622   # False Negatives\n","TN = 1959  # True Negatives (not used for F1, but included for completeness)\n","\n","# Calculate Precision and Recall\n","precision = TP / (TP + FP)\n","recall = TP / (TP + FN)\n","\n","# Calculate F1 Score\n","f1_score = 2 * (precision * recall) / (precision + recall)\n","\n","# Print results\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall (Sensitivity): {recall:.4f}\")\n","print(f\"F1 Score: {f1_score:.4f}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Testing Models"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Predicting sequences:   0%|          | 0/11949 [00:00<?, ?it/s]\n"]},{"ename":"ValueError","evalue":"Feature shape mismatch, expected: 334, got 320","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m corrected_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq, label \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mzip\u001b[39m(sequences, true_labels), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(sequences), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicting sequences\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 75\u001b[0m     preds, probs \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_bootstrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mesm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_converter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbootstrap_models\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# Skip invalid sequence and label\u001b[39;00m\n","Cell \u001b[0;32mIn[19], line 46\u001b[0m, in \u001b[0;36mpredict_bootstrap\u001b[0;34m(sequence, esm_model, batch_converter, bootstrap_models)\u001b[0m\n\u001b[1;32m     44\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m bootstrap_models:\n\u001b[0;32m---> 46\u001b[0m     prob \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_embedding\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     47\u001b[0m     pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(cls_embedding)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     48\u001b[0m     probs\u001b[38;5;241m.\u001b[39mappend(prob)\n","File \u001b[0;32m/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1606\u001b[0m, in \u001b[0;36mXGBClassifier.predict_proba\u001b[0;34m(self, X, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1604\u001b[0m     class_prob \u001b[38;5;241m=\u001b[39m softmax(raw_predt, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1605\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m class_prob\n\u001b[0;32m-> 1606\u001b[0m class_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mntree_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mntree_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1611\u001b[0m \u001b[43m    \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1612\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1613\u001b[0m \u001b[38;5;66;03m# If model is loaded from a raw booster there's no `n_classes_`\u001b[39;00m\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _cls_predict_proba(\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_classes_\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m), class_probs, np\u001b[38;5;241m.\u001b[39mvstack\n\u001b[1;32m   1616\u001b[0m )\n","File \u001b[0;32m/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1114\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[0;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1114\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[1;32m   1123\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n","File \u001b[0;32m/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/core.py:2268\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[0;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[1;32m   2264\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2265\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2266\u001b[0m         )\n\u001b[1;32m   2267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features() \u001b[38;5;241m!=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m-> 2268\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2269\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature shape mismatch, expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2270\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2271\u001b[0m         )\n\u001b[1;32m   2273\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m   2274\u001b[0m     _array_interface,\n\u001b[1;32m   2275\u001b[0m     _is_cudf_df,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2278\u001b[0m     _transform_pandas_df,\n\u001b[1;32m   2279\u001b[0m )\n\u001b[1;32m   2281\u001b[0m enable_categorical \u001b[38;5;241m=\u001b[39m _has_categorical(\u001b[38;5;28mself\u001b[39m, data)\n","\u001b[0;31mValueError\u001b[0m: Feature shape mismatch, expected: 334, got 320"]}],"source":["# Install if needed\n","# !pip install pandas numpy torch scikit-learn joblib tqdm\n","\n","import pandas as pd\n","import numpy as np\n","import joblib\n","import torch\n","from sklearn.metrics import accuracy_score, roc_auc_score\n","from esm import pretrained\n","import os\n","from tqdm import tqdm\n","\n","# === Load ESM2 model ===\n","def load_esm2_model():\n","    model, alphabet = pretrained.esm2_t6_8M_UR50D()\n","    model.eval()\n","    batch_converter = alphabet.get_batch_converter()\n","    return model, alphabet, batch_converter\n","\n","# === Load trained bootstrap models ===\n","def load_bootstrap_models(model_dir, base_model_name, n_models=5):\n","    models = []\n","    for i in range(1, n_models + 1):\n","        model_path = os.path.join(model_dir, f\"{base_model_name}_bootstrap_{i}.pkl\")\n","        models.append(joblib.load(model_path))\n","    return models\n","\n","# === Predict using bootstrap models ===\n","def predict_bootstrap(sequence, esm_model, batch_converter, bootstrap_models):\n","    sequence = sequence.strip().upper()\n","    valid_aa = set(\"ACDEFGHIKLMNPQRSTVWY\")\n","    if not all(aa in valid_aa for aa in sequence):\n","        return None, None  # Invalid sequence\n","\n","    data = [(\"protein\", sequence)]\n","    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n","\n","    with torch.no_grad():\n","        results = esm_model(batch_tokens, repr_layers=[6], return_contacts=False)\n","    token_representations = results[\"representations\"][6]\n","    cls_embedding = token_representations[0, 0, :].numpy().reshape(1, -1)\n","\n","    probs = []\n","    preds = []\n","    for model in bootstrap_models:\n","        prob = model.predict_proba(cls_embedding)[0][1]\n","        pred = model.predict(cls_embedding)[0]\n","        probs.append(prob)\n","        preds.append(pred)\n","\n","    return np.array(preds), np.array(probs)\n","\n","# === Main evaluation ===\n","# Paths and settings\n","csv_file = \"allergen_data_with_full_sequences.csv\"  # Path to your data\n","model_dir = \".\"  # Current folder\n","model_base_name = \"XGBoost_ESM-2_320dim_algpred2\"\n","n_bootstrap_models = 5\n","\n","# Load models\n","esm_model, alphabet, batch_converter = load_esm2_model()\n","bootstrap_models = load_bootstrap_models(model_dir, model_base_name, n_models=n_bootstrap_models)\n","\n","# Load sequences and labels\n","df = pd.read_csv(csv_file)\n","sequences = df[\"full_parent_protein_sequence\"].tolist()\n","true_labels = df[\"label\"].tolist()\n","\n","# Predict all\n","corrected_preds = []\n","corrected_probs = []\n","corrected_labels = []\n","\n","for seq, label in tqdm(zip(sequences, true_labels), total=len(sequences), desc=\"Predicting sequences\"):\n","    preds, probs = predict_bootstrap(seq, esm_model, batch_converter, bootstrap_models)\n","    if preds is None:\n","        continue  # Skip invalid sequence and label\n","    majority_vote = np.round(np.mean(preds))\n","    mean_prob = np.mean(probs)\n","    corrected_preds.append(majority_vote)\n","    corrected_probs.append(mean_prob)\n","    corrected_labels.append(label)\n","\n","# Evaluate\n","y_true = np.array(corrected_labels)\n","y_pred = np.array(corrected_preds)\n","y_proba = np.array(corrected_probs)\n","\n","acc = accuracy_score(y_true, y_pred)\n","auc = roc_auc_score(y_true, y_proba)\n","\n","print(f\"✅ Accuracy (Bootstrap Models): {acc:.4f}\")\n","print(f\"✅ ROC-AUC (Bootstrap Models): {auc:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["✅ Accuracy: 0.5790\n","✅ ROC-AUC: 0.5386\n"]}],"source":["\n","acc = accuracy_score(y_true, y_pred)\n","auc = roc_auc_score(y_true, y_proba)\n","\n","print(f\"✅ Accuracy: {acc:.4f}\")\n","print(f\"✅ ROC-AUC: {auc:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["(11945,)"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["y_pred.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Predicting with final model: 100%|██████████| 11949/11949 [16:35<00:00, 12.00it/s] "]},{"name":"stdout","output_type":"stream","text":["✅ Final Model Accuracy: 0.5540\n","✅ Final Model ROC-AUC: 0.4697\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# === Load the final original XGBoost model ===\n","final_model_path = \"XGBoost_ESM-2_320dim_algpred2_xgboost_model.pkl\"  # Adjust if needed\n","final_model = joblib.load(final_model_path)\n","\n","# === Predict all sequences with final model ===\n","corrected_preds_final = []\n","corrected_probs_final = []\n","corrected_labels_final = []\n","\n","for seq, label in tqdm(zip(sequences, true_labels), total=len(sequences), desc=\"Predicting with final model\"):\n","    seq = seq.strip().upper()\n","    valid_aa = set(\"ACDEFGHIKLMNPQRSTVWY\")\n","    if not all(aa in valid_aa for aa in seq):\n","        continue  # Skip sequence and label together\n","\n","    data = [(\"protein\", seq)]\n","    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n","\n","    with torch.no_grad():\n","        results = esm_model(batch_tokens, repr_layers=[6], return_contacts=False)\n","    token_representations = results[\"representations\"][6]\n","    cls_embedding = token_representations[0, 0, :].numpy().reshape(1, -1)\n","\n","    # Predict with the final model\n","    pred_final = final_model.predict(cls_embedding)[0]\n","    prob_final = final_model.predict_proba(cls_embedding)[0][1]\n","\n","    corrected_preds_final.append(pred_final)\n","    corrected_probs_final.append(prob_final)\n","    corrected_labels_final.append(label)\n","\n","# === Evaluate final model ===\n","y_true_final = np.array(corrected_labels_final)\n","y_pred_final = np.array(corrected_preds_final)\n","y_proba_final = np.array(corrected_probs_final)\n","\n","acc_final = accuracy_score(y_true_final, y_pred_final)\n","auc_final = roc_auc_score(y_true_final, y_proba_final)\n","\n","print(f\"✅ Final Model Accuracy: {acc_final:.4f}\")\n","print(f\"✅ Final Model ROC-AUC: {auc_final:.4f}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO8zzovwpYg5elCgDfk9yWE","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.21"}},"nbformat":4,"nbformat_minor":0}
