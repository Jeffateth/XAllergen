{"cells":[{"cell_type":"markdown","metadata":{},"source":["## imports"]},{"cell_type":"code","execution_count":212,"metadata":{},"outputs":[],"source":["#esm2_env\n","import pandas as pd\n","import numpy as np\n","import requests\n","from io import StringIO\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","import esm\n","from tqdm import tqdm\n","import sys\n","import csv\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","from sklearn.metrics import roc_auc_score, classification_report\n","from sklearn.dummy import DummyClassifier\n","from sklearn.linear_model import RidgeClassifier\n","import xgboost as xgb\n","import random\n","import zipfile\n","import io\n","import ipywidgets as widgets\n","from IPython.display import display, clear_output\n","import time\n","from contextlib import redirect_stdout, redirect_stderr\n","from transformers import BertModel, BertTokenizer\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## User input: Dataset "]},{"cell_type":"code","execution_count":214,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8fd7355230da48619dbc480676dd939b","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Dropdown(description='Select Dataset:', layout=Layout(width='350px'), options=('algpred2', 'alg‚Ä¶"]},"metadata":{},"output_type":"display_data"}],"source":["# IMPORTANT: \n","# 1. run the import cell and this one. \n","# 2. after choosing and pressing confirm in the widget run the rest of the code (run + arrow down in the cell options of the next cell)\n","# === Layout ===\n","layout = widgets.Layout(width='350px')\n","style = {'description_width': '150px'}\n","\n","# === Dropdowns ===\n","dataset_selector = widgets.Dropdown(\n","    options=[\"algpred2\", \"algpred2_resplit\", \"iedb\", \"AllergenAI\"],\n","    value=\"algpred2\",\n","    description=\"Select Dataset:\",\n","    layout=layout,\n","    style=style\n",")\n","\n","transformer_selector = widgets.Dropdown(\n","    options=[\"ESM-2_320dim\",\"ProtBert_1024dim\",\"ProtT5_1024dim\"],\n","    value=\"ESM-2_320dim\",\n","    description=\"Select Transformer:\",\n","    layout=layout,\n","    style=style\n",")\n","\n","model_selector = widgets.Dropdown(\n","    options=[\"XGBoost\", \"FFNN\",\"Ridge\"],\n","    value=\"XGBoost\",\n","    description=\"Select Model:\",\n","    layout=layout,\n","    style=style\n",")\n","\n","# === Output + Button ===\n","output = widgets.Output()\n","submit_button = widgets.Button(description=\"‚úÖ Confirm Selection\", button_style='success')\n","\n","# === Button callback ===\n","def on_button_clicked(b):\n","    global dataset_name, transformer_name, model_name  # <-- Add this\n","    dataset_name = dataset_selector.value\n","    transformer_name = transformer_selector.value\n","    model_name = model_selector.value\n","\n","    with output:\n","        clear_output()\n","        print(\"‚úÖ Selections made!\")\n","        print(f\"Dataset: {dataset_name}\")\n","        print(f\"Transformer: {transformer_name}\")\n","        print(f\"Model: {model_name}\")\n","\n","\n","submit_button.on_click(on_button_clicked)\n","\n","# === Display UI ===\n","display(widgets.VBox([\n","    dataset_selector,\n","    transformer_selector,\n","    model_selector,\n","    submit_button,\n","    output\n","]))\n"]},{"cell_type":"markdown","metadata":{},"source":["# Data curation"]},{"cell_type":"markdown","metadata":{},"source":["## Data from Benchmark"]},{"cell_type":"code","execution_count":200,"metadata":{},"outputs":[],"source":["# if dataset_name == \"benchmark\":\n","#     print('Benchmark dataset chosen')\n","\n","#     # Define base folder and output paths\n","#     data_dir = dataset_name\n","#     os.makedirs(data_dir, exist_ok=True)\n","\n","#     train_csv_path = os.path.join(data_dir, f\"{dataset_name}_train.csv\")\n","#     test_csv_path = os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","\n","#     # Check for existing files\n","#     # Skip if files already exist\n","#     if os.path.exists(train_csv_path) and os.path.exists(test_csv_path):\n","#         print(f\"‚è≠Ô∏è  Files already exist in '{data_dir}/'. Skipping FASTA parsing and CSV generation.\")\n","#     else:\n","#         # Define file-label mapping and GitHub raw URLs\n","#         base_url = \"https://raw.githubusercontent.com/Jeffateth/AllergenPredict/7fafbea0ab1646796abe40cafb800c46ba842bda/Benchmark_dataset\"\n","\n","#         datasets = {\n","#             \"train_p.fasta\": (1, \"train\"),\n","#             \"train_n.fasta\": (0, \"train\"),\n","#             \"test_p.fasta\":  (1, \"test\"),\n","#             \"test_n.fasta\":  (0, \"test\")\n","#         }\n","\n","#         # Parse FASTA format\n","#         def parse_fasta(fasta_text, label):\n","#             sequences = []\n","#             current_id = None\n","#             current_seq = \"\"\n","#             for line in fasta_text.strip().splitlines():\n","#                 line = line.strip()\n","#                 if line.startswith(\">\"):\n","#                     if current_id is not None:\n","#                         sequences.append((current_id, current_seq, label))\n","#                     current_id = line[1:]\n","#                     current_seq = \"\"\n","#                 else:\n","#                     current_seq += line\n","#             if current_id and current_seq:\n","#                 sequences.append((current_id, current_seq, label))\n","#             return sequences\n","\n","#         # Download and parse files\n","#         train_entries = []\n","#         test_entries = []\n","\n","#         for filename, (label, split) in datasets.items():\n","#             url = f\"{base_url}/{filename}\"\n","#             print(f\"‚¨áÔ∏è  Downloading {filename} from {url}...\")\n","#             response = requests.get(url)\n","#             response.raise_for_status()  # raise an error for failed downloads\n","\n","#             fasta_text = response.text\n","#             entries = parse_fasta(fasta_text, label)\n","#             if split == \"train\":\n","#                 train_entries.extend(entries)\n","#             else:\n","#                 test_entries.extend(entries)\n","\n","#         # Save to CSV inside dataset-named folder\n","#         df_train = pd.DataFrame(train_entries, columns=[\"id\", \"sequence\", \"label\"])\n","#         df_test = pd.DataFrame(test_entries, columns=[\"id\", \"sequence\", \"label\"])\n","\n","#         df_train.to_csv(train_csv_path, index=False)\n","#         df_test.to_csv(test_csv_path, index=False)\n","\n","#         print(f\"‚úÖ Saved training set to '{train_csv_path}'\")\n","#         print(f\"‚úÖ Saved testing set to '{test_csv_path}'\")\n","# else: print('Benchmark dataset not chosen')"]},{"cell_type":"markdown","metadata":{},"source":["## Data from IEDB"]},{"cell_type":"code","execution_count":201,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["IEDB dataset not chosen\n"]}],"source":["if dataset_name == 'iedb':\n","    print('IEDB dataset chosen')\n","\n","    # === CONFIG ===\n","    data_dir = dataset_name\n","    os.makedirs(data_dir, exist_ok=True)\n","\n","    # --- Load original CSV ---\n","    url = \"https://raw.githubusercontent.com/Jeffateth/AllergenPredict/b395c3276945b83ecc77513749361d6472706ca5/allergen_data_with_full_sequences.csv\"\n","    df = pd.read_csv(url)\n","\n","    # --- Prepare DataFrame ---\n","    df = df[[\"full_parent_protein_sequence\", \"label\"]].copy()\n","    df.rename(columns={\"full_parent_protein_sequence\": \"sequence\"}, inplace=True)\n","    df[\"id\"] = [f\"seq_{i}\" for i in range(len(df))]\n","    df = df[[\"id\", \"sequence\", \"label\"]]\n","\n","    # --- Train/test split (80/20 stratified) ---\n","    train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n","\n","    # --- Save splits ---\n","    train_df.to_csv(os.path.join(data_dir, f\"{dataset_name}_train.csv\"), index=False)\n","    test_df.to_csv(os.path.join(data_dir, f\"{dataset_name}_test.csv\"), index=False)\n","\n","    print(\"‚úÖ Data loaded, split, and saved for ESM2 embedding.\")\n","else: print('IEDB dataset not chosen')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data from AllergenAI "]},{"cell_type":"code","execution_count":202,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["AllergenAI dataset not chosen\n"]}],"source":["if dataset_name == 'AllergenAI':\n","    print('AllergenAI dataset chosen')\n","\n","    # === CONFIG ===\n","    data_dir = dataset_name\n","    os.makedirs(data_dir, exist_ok=True)\n","\n","    train_csv_path = os.path.join(data_dir, f\"{dataset_name}_train.csv\")\n","    test_csv_path = os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","\n","    # === Skip if already processed\n","    if os.path.exists(train_csv_path) and os.path.exists(test_csv_path):\n","        print(f\"‚è≠Ô∏è  Found existing train/test files in '{data_dir}/'. Skipping parsing.\")\n","    else:\n","        # === Standard amino acid order (1-letter codes)\n","        aa_letters = list(\"ACDEFGHIKLMNPQRSTVWY\")\n","\n","        # === Map one-hot vector to amino acid letter\n","        onehot_to_aa = {\n","            tuple(1 if i == j else 0 for i in range(20)): aa\n","            for j, aa in enumerate(aa_letters)\n","        }\n","\n","        def load_onehot_file(filepath, label):\n","            \"\"\"Converts one-hot file to list of (sequence, label)\"\"\"\n","            data = np.loadtxt(filepath)\n","            sequences = []\n","            current = []\n","\n","            for row in data:\n","                if np.all(row == 0):\n","                    if current:\n","                        sequences.append((\"\".join(current), label))\n","                        current = []\n","                else:\n","                    aa = onehot_to_aa.get(tuple(int(x) for x in row))\n","                    if aa:\n","                        current.append(aa)\n","                    else:\n","                        raise ValueError(f\"Unknown one-hot vector: {row}\")\n","\n","            if current:\n","                sequences.append((\"\".join(current), label))\n","\n","            return sequences\n","\n","        # === Load both files ===\n","        positive_sequences = load_onehot_file(\"pos.txt\", label=1)\n","        negative_sequences = load_onehot_file(\"neg.txt\", label=0)\n","\n","        # === Combine and format as DataFrame\n","        all_sequences = positive_sequences + negative_sequences\n","        df = pd.DataFrame(all_sequences, columns=[\"sequence\", \"label\"])\n","        df[\"id\"] = [f\"seq_{i}\" for i in range(len(df))]\n","        df = df[[\"id\", \"sequence\", \"label\"]]\n","\n","        # === Split into train/test (80/20 stratified)\n","        train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\n","\n","        # === Save CSVs\n","        train_df.to_csv(train_csv_path, index=False)\n","        test_df.to_csv(test_csv_path, index=False)\n","\n","        print(f\"‚úÖ Saved training set to '{train_csv_path}'\")\n","        print(f\"‚úÖ Saved testing set to '{test_csv_path}'\")\n","else:\n","    print('AllergenAI dataset not chosen')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data from AlgPred 2.0"]},{"cell_type":"code","execution_count":203,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9444,"status":"ok","timestamp":1743545518910,"user":{"displayName":"Jeff","userId":"15773939950998775573"},"user_tz":-120},"id":"ilcsg8tr0Nus","outputId":"6b11ab8a-9c74-4ba9-f7fb-5b59a4090162"},"outputs":[{"name":"stdout","output_type":"stream","text":["AlgPred 2.0 dataset chosen\n","‚è≠Ô∏è  Files already exist. Skipping download and parsing.\n"]}],"source":["if dataset_name in [\"algpred2\", \"algpred2_resplit\"]:\n","    print('AlgPred 2.0 dataset chosen')\n","    data_dir = dataset_name\n","    os.makedirs(data_dir, exist_ok=True)\n","\n","    train_csv_path = os.path.join(data_dir, f\"{dataset_name}_train.csv\")\n","    test_csv_path = os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","\n","    # Skip if both files already exist\n","    if os.path.exists(train_csv_path) and os.path.exists(test_csv_path):\n","        print(\"‚è≠Ô∏è  Files already exist. Skipping download and parsing.\")\n","    else:\n","        # URLs from AlgPred 2.0\n","        datasets = {\n","            \"train_positive\": (\"https://webs.iiitd.edu.in/raghava/algpred2/datasets/train_positive.txt\", 1, \"train\"),\n","            \"train_negative\": (\"https://webs.iiitd.edu.in/raghava/algpred2/datasets/train_negative.txt\", 0, \"train\"),\n","            \"validation_positive\": (\"https://webs.iiitd.edu.in/raghava/algpred2/datasets/validation_positive.txt\", 1, \"val\"),\n","            \"validation_negative\": (\"https://webs.iiitd.edu.in/raghava/algpred2/datasets/validation_negative.txt\", 0, \"val\")\n","        }\n","\n","        def parse_fasta(fasta_text, label):\n","            sequences = []\n","            current_id = None\n","            current_seq = \"\"\n","            for line in fasta_text.strip().splitlines():\n","                line = line.strip()\n","                if line.startswith(\">\"):\n","                    if current_id is not None:\n","                        sequences.append((current_id, current_seq, label))\n","                    current_id = line[1:]\n","                    current_seq = \"\"\n","                else:\n","                    current_seq += line\n","            if current_id and current_seq:\n","                sequences.append((current_id, current_seq, label))\n","            return sequences\n","\n","        train_entries = []\n","        val_entries = []\n","\n","        for name, (url, label, split) in datasets.items():\n","            print(f\"‚¨áÔ∏è  Downloading {name} from {url}...\")\n","            response = requests.get(url)\n","            entries = parse_fasta(response.text, label)\n","            if split == \"train\":\n","                train_entries.extend(entries)\n","            else:\n","                val_entries.extend(entries)\n","\n","        df_train = pd.DataFrame(train_entries, columns=[\"id\", \"sequence\", \"label\"])\n","        df_val = pd.DataFrame(val_entries, columns=[\"id\", \"sequence\", \"label\"])\n","\n","        df_train.to_csv(train_csv_path, index=False)\n","        df_val.to_csv(test_csv_path, index=False)\n","\n","        print(f\"‚úÖ Saved training set to '{train_csv_path}'\")\n","        print(f\"‚úÖ Saved validation set to '{test_csv_path}'\")\n","else:\n","    print('AlgPred 2.0 dataset not chosen')\n"]},{"cell_type":"markdown","metadata":{},"source":["## algpred 2 resplitting"]},{"cell_type":"code","execution_count":204,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["‚ÑπÔ∏è  Not running resplit because algpred2 is not the selected dataset.\n"]}],"source":["if dataset_name == \"algpred2_resplit\":\n","    print(\"üîÑ Resplitting AlgPred 2.0 dataset (80/20)...\")\n","\n","    data_dir = dataset_name\n","    train_csv_path = os.path.join(data_dir, f\"{dataset_name}_train.csv\")\n","    test_csv_path = os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","\n","    # Load existing CSVs\n","    df_train = pd.read_csv(train_csv_path)\n","    df_test = pd.read_csv(test_csv_path)\n","\n","    # Combine and shuffle\n","    combined_df = pd.concat([df_train, df_test], ignore_index=True)\n","    combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n","\n","    # Resplit 80/20 stratified\n","    new_train_df, new_test_df = train_test_split(\n","        combined_df, test_size=0.2, stratify=combined_df[\"label\"], random_state=42\n","    )\n","\n","    # Save resplit data\n","    new_train_path = os.path.join(data_dir, f\"{dataset_name}_train.csv\")\n","    new_test_path = os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","\n","    new_train_df.to_csv(new_train_path, index=False)\n","    new_test_df.to_csv(new_test_path, index=False)\n","\n","    print(f\"‚úÖ New training set saved to '{new_train_path}'\")\n","    print(f\"‚úÖ New testing set saved to '{new_test_path}'\")\n","else:\n","    print(\"‚ÑπÔ∏è  Not running resplit because algpred2 is not the selected dataset.\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Embedding generation"]},{"cell_type":"markdown","metadata":{},"source":["## ESM-2 embedding extraction"]},{"cell_type":"code","execution_count":205,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":206837,"status":"ok","timestamp":1743546270778,"user":{"displayName":"Jeff","userId":"15773939950998775573"},"user_tz":-120},"id":"1_Jv62ty0mrA","outputId":"778a1e22-2458-4988-a7f1-e5b0c49097ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["ESM-2 embedding generation not chosen\n"]}],"source":["if transformer_name == 'ESM-2_320dim':\n","    print('ESM-2 embedding extraction chosen')\n","    # === CONFIG ===\n","    feature_dim = 320           # ESM-2 T6-8M embedding size\n","    batch_size = 1              # Adjust based on memory\n","    data_dir = dataset_name     # All files live in a folder named after the dataset\n","\n","    # --- Ensure directory exists ---\n","    os.makedirs(data_dir, exist_ok=True)\n","\n","    # --- Construct dynamic file paths ---\n","    input_files = {\n","        \"train\": os.path.join(data_dir, f\"{dataset_name}_train.csv\"),\n","        \"test\": os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","    }\n","\n","    # --- Output file paths ---\n","    embedding_files = {\n","        \"train\": os.path.join(data_dir, f\"train_{dataset_name}_esm2_embeddings.csv\"),\n","        \"test\": os.path.join(data_dir, f\"test_{dataset_name}_esm2_embeddings.csv\")\n","    }\n","\n","    # Check if both embedding files exist\n","    if all(os.path.exists(f) for f in embedding_files.values()):\n","        print(f\"‚úÖ ESM2 embedding files already exist in '{data_dir}/'. Skipping embedding generation.\")\n","    else:\n","        # --- Load ESM-2 model ---\n","        model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n","        batch_converter = alphabet.get_batch_converter()\n","        model.eval()\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model = model.to(device)\n","\n","        # --- Helper function ---\n","        def process_file(split_name, input_file):\n","            temp_file = os.path.join(data_dir, f\"{split_name}_{dataset_name}_esm2_embeddings_temp.csv\")\n","            final_file = os.path.join(data_dir, f\"{split_name}_{dataset_name}_esm2_embeddings.csv\")\n","\n","            # Load dataset\n","            df = pd.read_csv(input_file)\n","            sequences = list(df[\"sequence\"])\n","            labels = list(df[\"label\"])\n","            ids = list(df[\"id\"])\n","\n","            # Resume support\n","            if os.path.exists(temp_file):\n","                processed_ids = set(pd.read_csv(temp_file, usecols=[\"id\"])[\"id\"])\n","                print(f\"üîÅ Resuming {split_name} from {temp_file} ‚Äî {len(processed_ids)} entries already processed.\")\n","            else:\n","                processed_ids = set()\n","\n","            remaining_data = [(ids[i], sequences[i], labels[i]) for i in range(len(ids)) if ids[i] not in processed_ids]\n","\n","            # Output format\n","            fieldnames = [\"id\", \"label\"] + [f\"f{k}\" for k in range(feature_dim)]\n","            write_header = not os.path.exists(temp_file)\n","\n","            print(f\"‚öôÔ∏è  Extracting embeddings for {split_name} set... ({len(remaining_data)} sequences remaining)\")\n","\n","            with open(temp_file, mode=\"a\", newline=\"\") as f:\n","                writer = csv.DictWriter(f, fieldnames=fieldnames)\n","                if write_header:\n","                    writer.writeheader()\n","\n","                for i in tqdm(range(0, len(remaining_data), batch_size)):\n","                    batch = remaining_data[i:i + batch_size]\n","                    batch_ids = [x[0] for x in batch]\n","                    batch_seqs = [x[1] for x in batch]\n","                    batch_labels = [x[2] for x in batch]\n","\n","                    batch_data = [(batch_ids[j], batch_seqs[j]) for j in range(len(batch_seqs))]\n","                    _, _, batch_tokens = batch_converter(batch_data)\n","                    batch_tokens = batch_tokens.to(device)\n","\n","                    with torch.no_grad():\n","                        outputs = model(batch_tokens, repr_layers=[6])\n","                        token_representations = outputs[\"representations\"][6]\n","\n","                    rows = []\n","                    for j, (_, seq) in enumerate(batch_data):\n","                        representation = token_representations[j, 1:len(seq)+1].mean(0)\n","                        entry = {\n","                            \"id\": batch_ids[j],\n","                            \"label\": batch_labels[j],\n","                        }\n","                        for k in range(feature_dim):\n","                            entry[f\"f{k}\"] = representation[k].item()\n","                        rows.append(entry)\n","\n","                    writer.writerows(rows)\n","\n","            # Final save\n","            os.replace(temp_file, final_file)\n","            print(f\"‚úÖ Final {split_name} embeddings saved to '{final_file}'\")\n","\n","        # --- Process each split ---\n","        for split, file in input_files.items():\n","            process_file(split, file)\n","else: print('ESM-2 embedding generation not chosen')"]},{"cell_type":"markdown","metadata":{},"source":["## ProtBert Embeddings "]},{"cell_type":"code","execution_count":206,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ProtBert embedding extraction chosen\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"637d7f90393d4014a826eaeb46131385","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   6%|5         | 94.4M/1.68G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1bb12e4205f24c7dad79d41e88999af8","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/1.68G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["‚öôÔ∏è  Extracting ProtBert embeddings for train... (16120 sequences)\n"]},{"name":"stderr","output_type":"stream","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16120/16120 [8:22:02<00:00,  1.87s/it]\n"]},{"name":"stdout","output_type":"stream","text":["‚úÖ Final train embeddings saved to 'algpred2/train_algpred2_protbert_embeddings.csv'\n","‚öôÔ∏è  Extracting ProtBert embeddings for test... (4030 sequences)\n"]},{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4030/4030 [4:07:01<00:00,  3.68s/it]     "]},{"name":"stdout","output_type":"stream","text":["‚úÖ Final test embeddings saved to 'algpred2/test_algpred2_protbert_embeddings.csv'\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["if transformer_name == \"ProtBert_1024dim\":\n","    print('ProtBert embedding extraction chosen')\n","\n","    \n","\n","    # === CONFIG ===\n","    feature_dim = 1024\n","    batch_size = 1\n","    data_dir = dataset_name\n","    os.makedirs(data_dir, exist_ok=True)\n","\n","    input_files = {\n","        \"train\": os.path.join(data_dir, f\"{dataset_name}_train.csv\"),\n","        \"test\": os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","    }\n","\n","    embedding_files = {\n","        \"train\": os.path.join(data_dir, f\"train_{dataset_name}_protbert_embeddings.csv\"),\n","        \"test\": os.path.join(data_dir, f\"test_{dataset_name}_protbert_embeddings.csv\")\n","    }\n","\n","    if all(os.path.exists(f) for f in embedding_files.values()):\n","        print(f\"‚úÖ ProtBert embedding files already exist in '{data_dir}/'. Skipping generation.\")\n","    else:\n","        # === Load ProtBert ===\n","        tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert\", do_lower_case=False)\n","        model = BertModel.from_pretrained(\"Rostlab/prot_bert\")\n","        model.eval()\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model = model.to(device)\n","\n","        def process_file(split_name, input_file):\n","            temp_file = os.path.join(data_dir, f\"{split_name}_{dataset_name}_protbert_embeddings_temp.csv\")\n","            final_file = os.path.join(data_dir, f\"{split_name}_{dataset_name}_protbert_embeddings.csv\")\n","\n","            df = pd.read_csv(input_file)\n","            sequences = list(df[\"sequence\"])\n","            labels = list(df[\"label\"])\n","            ids = list(df[\"id\"])\n","\n","            if os.path.exists(temp_file):\n","                processed_ids = set(pd.read_csv(temp_file, usecols=[\"id\"])[\"id\"])\n","                print(f\"üîÅ Resuming {split_name} from {temp_file} ‚Äî {len(processed_ids)} already processed.\")\n","            else:\n","                processed_ids = set()\n","\n","            remaining_data = [(ids[i], sequences[i], labels[i]) for i in range(len(ids)) if ids[i] not in processed_ids]\n","\n","            fieldnames = [\"id\", \"label\"] + [f\"f{k}\" for k in range(feature_dim)]\n","            write_header = not os.path.exists(temp_file)\n","\n","            print(f\"‚öôÔ∏è  Extracting ProtBert embeddings for {split_name}... ({len(remaining_data)} sequences)\")\n","\n","            with open(temp_file, mode=\"a\", newline=\"\") as f:\n","                writer = csv.DictWriter(f, fieldnames=fieldnames)\n","                if write_header:\n","                    writer.writeheader()\n","\n","                for i in tqdm(range(0, len(remaining_data), batch_size)):\n","                    batch = remaining_data[i:i + batch_size]\n","                    batch_ids = [x[0] for x in batch]\n","                    batch_seqs = [x[1] for x in batch]\n","                    batch_labels = [x[2] for x in batch]\n","\n","                    # Preprocess for ProtBert\n","                    batch_seqs = [\" \".join(list(seq)) for seq in batch_seqs]\n","                    encoded_input = tokenizer(batch_seqs, return_tensors='pt', padding=True, truncation=True)\n","                    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n","\n","                    with torch.no_grad():\n","                        output = model(**encoded_input)\n","                        embeddings = output.last_hidden_state.mean(dim=1)\n","\n","                    rows = []\n","                    for j in range(len(batch)):\n","                        entry = {\n","                            \"id\": batch_ids[j],\n","                            \"label\": batch_labels[j],\n","                        }\n","                        for k in range(feature_dim):\n","                            entry[f\"f{k}\"] = embeddings[j][k].item()\n","                        rows.append(entry)\n","\n","                    writer.writerows(rows)\n","\n","            os.replace(temp_file, final_file)\n","            print(f\"‚úÖ Final {split_name} embeddings saved to '{final_file}'\")\n","\n","        for split, file in input_files.items():\n","            process_file(split, file)\n","else:\n","    print('ProtBert embedding generation not chosen')\n"]},{"cell_type":"markdown","metadata":{},"source":["## Prot T5 Embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if transformer_name == \"ProtT5_1024dim\":\n","    print('ProtT5 embedding extraction chosen')\n","\n","    # === CONFIG ===\n","    feature_dim = 1024\n","    batch_size = 1\n","    data_dir = dataset_name\n","    os.makedirs(data_dir, exist_ok=True)\n","\n","    input_files = {\n","        \"train\": os.path.join(data_dir, f\"{dataset_name}_train.csv\"),\n","        \"test\": os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","    }\n","\n","    embedding_files = {\n","        \"train\": os.path.join(data_dir, f\"train_{dataset_name}_prott5_embeddings.csv\"),\n","        \"test\": os.path.join(data_dir, f\"test_{dataset_name}_prott5_embeddings.csv\")\n","    }\n","\n","    if all(os.path.exists(f) for f in embedding_files.values()):\n","        print(f\"‚úÖ ProtT5 embedding files already exist in '{data_dir}/'. Skipping generation.\")\n","    else:\n","        # === Load ProtT5 Encoder ===\n","        from transformers import T5Tokenizer, T5EncoderModel\n","\n","        tokenizer = T5Tokenizer.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\", do_lower_case=False)\n","        model = T5EncoderModel.from_pretrained(\"Rostlab/prot_t5_xl_uniref50\")\n","        model.eval()\n","        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        model = model.to(device)\n","\n","        def process_file(split_name, input_file):\n","            temp_file = os.path.join(data_dir, f\"{split_name}_{dataset_name}_prott5_embeddings_temp.csv\")\n","            final_file = os.path.join(data_dir, f\"{split_name}_{dataset_name}_prott5_embeddings.csv\")\n","\n","            df = pd.read_csv(input_file)\n","            sequences = list(df[\"sequence\"])\n","            labels = list(df[\"label\"])\n","            ids = list(df[\"id\"])\n","\n","            if os.path.exists(temp_file):\n","                processed_ids = set(pd.read_csv(temp_file, usecols=[\"id\"])[\"id\"])\n","                print(f\"üîÅ Resuming {split_name} from {temp_file} ‚Äî {len(processed_ids)} already processed.\")\n","            else:\n","                processed_ids = set()\n","\n","            remaining_data = [(ids[i], sequences[i], labels[i]) for i in range(len(ids)) if ids[i] not in processed_ids]\n","\n","            fieldnames = [\"id\", \"label\"] + [f\"f{k}\" for k in range(feature_dim)]\n","            write_header = not os.path.exists(temp_file)\n","\n","            print(f\"‚öôÔ∏è  Extracting ProtT5 embeddings for {split_name}... ({len(remaining_data)} sequences)\")\n","\n","            with open(temp_file, mode=\"a\", newline=\"\") as f:\n","                writer = csv.DictWriter(f, fieldnames=fieldnames)\n","                if write_header:\n","                    writer.writeheader()\n","\n","                for i in tqdm(range(0, len(remaining_data), batch_size)):\n","                    batch = remaining_data[i:i + batch_size]\n","                    batch_ids = [x[0] for x in batch]\n","                    batch_seqs = [x[1] for x in batch]\n","                    batch_labels = [x[2] for x in batch]\n","\n","                    # Preprocess for ProtT5\n","                    batch_seqs = [\" \".join(list(seq)) for seq in batch_seqs]\n","                    encoded_input = tokenizer(batch_seqs, return_tensors='pt', padding=True, truncation=True)\n","                    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n","\n","                    with torch.no_grad():\n","                        output = model(**encoded_input)\n","                        embeddings = output.last_hidden_state.mean(dim=1)\n","\n","                    rows = []\n","                    for j in range(len(batch)):\n","                        entry = {\n","                            \"id\": batch_ids[j],\n","                            \"label\": batch_labels[j],\n","                        }\n","                        for k in range(feature_dim):\n","                            entry[f\"f{k}\"] = embeddings[j][k].item()\n","                        rows.append(entry)\n","\n","                    writer.writerows(rows)\n","\n","            os.replace(temp_file, final_file)\n","            print(f\"‚úÖ Final {split_name} embeddings saved to '{final_file}'\")\n","\n","        for split, file in input_files.items():\n","            process_file(split, file)\n","else:\n","    print('ProtT5 embedding generation not chosen')\n"]},{"cell_type":"markdown","metadata":{},"source":["# concatanate ESM fold data with 3d DSSP data into vector"]},{"cell_type":"code","execution_count":207,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(3915, 336)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>label</th>\n","      <th>f0</th>\n","      <th>f1</th>\n","      <th>f2</th>\n","      <th>f3</th>\n","      <th>f4</th>\n","      <th>f5</th>\n","      <th>f6</th>\n","      <th>f7</th>\n","      <th>...</th>\n","      <th>psi_mean</th>\n","      <th>psi_std</th>\n","      <th>SS_H</th>\n","      <th>SS_E</th>\n","      <th>SS_G</th>\n","      <th>SS_I</th>\n","      <th>SS_B</th>\n","      <th>SS_T</th>\n","      <th>SS_S</th>\n","      <th>SS_-</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>P_13</td>\n","      <td>1</td>\n","      <td>-0.109098</td>\n","      <td>-0.185716</td>\n","      <td>0.221519</td>\n","      <td>0.117864</td>\n","      <td>0.173010</td>\n","      <td>-0.071297</td>\n","      <td>0.091311</td>\n","      <td>0.057937</td>\n","      <td>...</td>\n","      <td>67.815480</td>\n","      <td>75.453660</td>\n","      <td>83</td>\n","      <td>139</td>\n","      <td>24</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>57</td>\n","      <td>40</td>\n","      <td>218</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>P_14</td>\n","      <td>1</td>\n","      <td>-0.130675</td>\n","      <td>0.124254</td>\n","      <td>0.189468</td>\n","      <td>0.133837</td>\n","      <td>0.307257</td>\n","      <td>0.215280</td>\n","      <td>0.131589</td>\n","      <td>-0.020784</td>\n","      <td>...</td>\n","      <td>53.922152</td>\n","      <td>94.515936</td>\n","      <td>37</td>\n","      <td>71</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>19</td>\n","      <td>8</td>\n","      <td>15</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>P_17</td>\n","      <td>1</td>\n","      <td>-0.047639</td>\n","      <td>0.091742</td>\n","      <td>0.206646</td>\n","      <td>0.098816</td>\n","      <td>0.142606</td>\n","      <td>-0.018436</td>\n","      <td>-0.028555</td>\n","      <td>0.027641</td>\n","      <td>...</td>\n","      <td>-22.113768</td>\n","      <td>51.173919</td>\n","      <td>109</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>P_46</td>\n","      <td>1</td>\n","      <td>-0.186663</td>\n","      <td>0.066537</td>\n","      <td>0.203491</td>\n","      <td>0.166804</td>\n","      <td>0.384989</td>\n","      <td>-0.193996</td>\n","      <td>-0.038884</td>\n","      <td>-0.133102</td>\n","      <td>...</td>\n","      <td>48.047500</td>\n","      <td>93.955165</td>\n","      <td>32</td>\n","      <td>66</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>25</td>\n","      <td>11</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>P_47</td>\n","      <td>1</td>\n","      <td>-0.181057</td>\n","      <td>0.059647</td>\n","      <td>0.199809</td>\n","      <td>0.173463</td>\n","      <td>0.381596</td>\n","      <td>-0.214031</td>\n","      <td>-0.020569</td>\n","      <td>-0.122544</td>\n","      <td>...</td>\n","      <td>49.051250</td>\n","      <td>93.914859</td>\n","      <td>32</td>\n","      <td>66</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>26</td>\n","      <td>9</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3910</th>\n","      <td>P_2644</td>\n","      <td>1</td>\n","      <td>-0.135910</td>\n","      <td>0.127915</td>\n","      <td>0.055814</td>\n","      <td>0.096571</td>\n","      <td>-0.036245</td>\n","      <td>0.172999</td>\n","      <td>0.011741</td>\n","      <td>-0.130888</td>\n","      <td>...</td>\n","      <td>-7.963262</td>\n","      <td>63.282675</td>\n","      <td>426</td>\n","      <td>0</td>\n","      <td>19</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>45</td>\n","      <td>27</td>\n","      <td>80</td>\n","    </tr>\n","    <tr>\n","      <th>3911</th>\n","      <td>P_2649</td>\n","      <td>1</td>\n","      <td>-0.297236</td>\n","      <td>0.020432</td>\n","      <td>-0.007142</td>\n","      <td>-0.062785</td>\n","      <td>0.202298</td>\n","      <td>0.153170</td>\n","      <td>0.047626</td>\n","      <td>0.001273</td>\n","      <td>...</td>\n","      <td>10.300000</td>\n","      <td>76.673221</td>\n","      <td>60</td>\n","      <td>8</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>11</td>\n","      <td>11</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>3912</th>\n","      <td>P_2655</td>\n","      <td>1</td>\n","      <td>-0.212389</td>\n","      <td>0.150151</td>\n","      <td>0.042394</td>\n","      <td>0.079228</td>\n","      <td>-0.026983</td>\n","      <td>0.218209</td>\n","      <td>0.066965</td>\n","      <td>-0.115486</td>\n","      <td>...</td>\n","      <td>-6.842010</td>\n","      <td>64.090766</td>\n","      <td>423</td>\n","      <td>0</td>\n","      <td>14</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>49</td>\n","      <td>27</td>\n","      <td>89</td>\n","    </tr>\n","    <tr>\n","      <th>3913</th>\n","      <td>P_2671</td>\n","      <td>1</td>\n","      <td>-0.342869</td>\n","      <td>0.054191</td>\n","      <td>0.031891</td>\n","      <td>-0.067981</td>\n","      <td>0.154429</td>\n","      <td>0.175938</td>\n","      <td>0.044891</td>\n","      <td>0.027024</td>\n","      <td>...</td>\n","      <td>12.296330</td>\n","      <td>78.464713</td>\n","      <td>60</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>16</td>\n","      <td>6</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>3914</th>\n","      <td>P_2682</td>\n","      <td>1</td>\n","      <td>-0.186285</td>\n","      <td>0.139882</td>\n","      <td>0.101441</td>\n","      <td>0.118745</td>\n","      <td>-0.052281</td>\n","      <td>0.202712</td>\n","      <td>0.036769</td>\n","      <td>-0.117572</td>\n","      <td>...</td>\n","      <td>-7.749671</td>\n","      <td>63.618326</td>\n","      <td>432</td>\n","      <td>0</td>\n","      <td>13</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>50</td>\n","      <td>31</td>\n","      <td>82</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3915 rows √ó 336 columns</p>\n","</div>"],"text/plain":["          id  label        f0        f1        f2        f3        f4  \\\n","0       P_13      1 -0.109098 -0.185716  0.221519  0.117864  0.173010   \n","1       P_14      1 -0.130675  0.124254  0.189468  0.133837  0.307257   \n","2       P_17      1 -0.047639  0.091742  0.206646  0.098816  0.142606   \n","3       P_46      1 -0.186663  0.066537  0.203491  0.166804  0.384989   \n","4       P_47      1 -0.181057  0.059647  0.199809  0.173463  0.381596   \n","...      ...    ...       ...       ...       ...       ...       ...   \n","3910  P_2644      1 -0.135910  0.127915  0.055814  0.096571 -0.036245   \n","3911  P_2649      1 -0.297236  0.020432 -0.007142 -0.062785  0.202298   \n","3912  P_2655      1 -0.212389  0.150151  0.042394  0.079228 -0.026983   \n","3913  P_2671      1 -0.342869  0.054191  0.031891 -0.067981  0.154429   \n","3914  P_2682      1 -0.186285  0.139882  0.101441  0.118745 -0.052281   \n","\n","            f5        f6        f7  ...   psi_mean    psi_std  SS_H  SS_E  \\\n","0    -0.071297  0.091311  0.057937  ...  67.815480  75.453660    83   139   \n","1     0.215280  0.131589 -0.020784  ...  53.922152  94.515936    37    71   \n","2    -0.018436 -0.028555  0.027641  ... -22.113768  51.173919   109     0   \n","3    -0.193996 -0.038884 -0.133102  ...  48.047500  93.955165    32    66   \n","4    -0.214031 -0.020569 -0.122544  ...  49.051250  93.914859    32    66   \n","...        ...       ...       ...  ...        ...        ...   ...   ...   \n","3910  0.172999  0.011741 -0.130888  ...  -7.963262  63.282675   426     0   \n","3911  0.153170  0.047626  0.001273  ...  10.300000  76.673221    60     8   \n","3912  0.218209  0.066965 -0.115486  ...  -6.842010  64.090766   423     0   \n","3913  0.175938  0.044891  0.027024  ...  12.296330  78.464713    60     8   \n","3914  0.202712  0.036769 -0.117572  ...  -7.749671  63.618326   432     0   \n","\n","      SS_G  SS_I  SS_B  SS_T  SS_S  SS_-  \n","0       24     0     1    57    40   218  \n","1        3     5     0    19     8    15  \n","2        6     0     0     3     7    13  \n","3        4     6     0    25    11    16  \n","4        4     6     0    26     9    17  \n","...    ...   ...   ...   ...   ...   ...  \n","3910    19    10     0    45    27    80  \n","3911     3     0     0    11    11    16  \n","3912    14     5     0    49    27    89  \n","3913     0     0     2    16     6    17  \n","3914    13     0     0    50    31    82  \n","\n","[3915 rows x 336 columns]"]},"execution_count":207,"metadata":{},"output_type":"execute_result"}],"source":["# Read the two DataFrames\n","df_original = pd.read_csv('algpred2/train_algpred2_esm2_embeddings.csv')\n","df_add = pd.read_csv('dssp_features.csv')\n","\n","# Merge on matching protein IDs (df_original 'id' <-> df_add 'protein')\n","df_merged = pd.merge(df_original, df_add, left_on='id', right_on='protein', how='inner')\n","\n","# (Optional) drop the redundant 'protein' column if you no longer need it\n","df_merged = df_merged.drop(columns=['protein'])\n","\n","# Show the first few rows of the merged DataFrame\n","print(df_merged.shape)\n","df_merged"]},{"cell_type":"markdown","metadata":{},"source":["# Models"]},{"cell_type":"markdown","metadata":{},"source":["## Ridge Regression"]},{"cell_type":"code","execution_count":208,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Ridge model not chosen\n"]}],"source":["if model_name == 'Ridge':\n","    log_filename = f\"{model_name}_{transformer_name}_{dataset_name}_output.txt\"\n","    log_path = os.path.join(\"logs\", log_filename)\n","    os.makedirs(\"logs\", exist_ok=True)\n","\n","    class Tee:\n","        def __init__(self, *streams):\n","            self.streams = streams\n","        def write(self, text):\n","            for stream in self.streams:\n","                stream.write(text)\n","                stream.flush()\n","        def flush(self):\n","            for stream in self.streams:\n","                stream.flush()\n","\n","    with open(log_path, \"w\") as log_file:\n","        tee = Tee(sys.stdout, log_file)\n","        with redirect_stdout(tee), redirect_stderr(tee):\n","\n","            print('Ridge Classifier model chosen')\n","            print(dataset_name, 'dataset chosen')\n","\n","            data_dir = dataset_name\n","            embedding_files = {\n","                \"train\": os.path.join(data_dir, f\"train_{dataset_name}_esm2_embeddings.csv\"),\n","                \"test\": os.path.join(data_dir, f\"test_{dataset_name}_esm2_embeddings.csv\")\n","            }\n","\n","            df_train = pd.read_csv(embedding_files[\"train\"])\n","            df_test = pd.read_csv(embedding_files[\"test\"])\n","\n","            feature_cols = [f\"f{i}\" for i in range(320)]\n","            X_train_full = df_train[feature_cols].values\n","            y_train_full = df_train[\"label\"].values\n","            X_test = df_test[feature_cols].values\n","            y_test = df_test[\"label\"].values\n","\n","            print(f\"‚úÖ Loaded: Train={X_train_full.shape}, Test={X_test.shape}\")\n","\n","            print(\"\\nüìâ DummyClassifier (Stratified) on Training Set (CV):\\n\")\n","            dummy = DummyClassifier(strategy=\"stratified\", random_state=42)\n","            dummy_aucs = []\n","            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","            for train_idx, val_idx in cv.split(X_train_full, y_train_full):\n","                dummy.fit(X_train_full[train_idx], y_train_full[train_idx])\n","                y_dummy_proba = dummy.predict_proba(X_train_full[val_idx])[:, 1]\n","                auc = roc_auc_score(y_train_full[val_idx], y_dummy_proba)\n","                dummy_aucs.append(auc)\n","\n","            print(f\"üìä Dummy ROC-AUC: {np.mean(dummy_aucs):.4f} ¬± {np.std(dummy_aucs):.4f}\")\n","\n","            print(\"\\nüöÄ 5-Fold Cross-Validation (RidgeClassifier) on Training Set...\\n\")\n","            ridge_aucs = []\n","            for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_full, y_train_full)):\n","                X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]\n","                y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n","\n","                clf = RidgeClassifier()\n","                clf.fit(X_train, y_train)\n","\n","                y_pred = clf.predict(X_val)\n","                auc = roc_auc_score(y_val, y_pred)\n","                ridge_aucs.append(auc)\n","\n","                print(f\"üìÇ Fold {fold+1} AUC: {auc:.4f}\")\n","                print(classification_report(y_val, y_pred, digits=4))\n","                print(\"------\")\n","\n","            mean_auc = np.mean(ridge_aucs)\n","            std_auc = np.std(ridge_aucs, ddof=1)\n","            se_auc = std_auc / np.sqrt(len(ridge_aucs))\n","            print(f\"\\n‚úÖ Mean CV ROC-AUC: {mean_auc:.4f} ¬± {std_auc:.4f} (SE = {se_auc:.4f})\")\n","\n","            print(\"\\nüîí Final Evaluation on Hold-Out Test Set...\\n\")\n","            clf_final = RidgeClassifier()\n","            clf_final.fit(X_train_full, y_train_full)\n","\n","            y_test_pred = clf_final.predict(X_test)\n","            test_auc = roc_auc_score(y_test, y_test_pred)\n","\n","            print(classification_report(y_test, y_test_pred, digits=4))\n","            print(f\"üéØ Final Test ROC-AUC: {test_auc:.4f}\")\n","\n","            print(\"\\nüß™ Y-Scrambling (sanity check) on Training Set...\\n\")\n","            y_scrambled = y_train_full.copy()\n","            random.seed(42)\n","            random.shuffle(y_scrambled)\n","\n","            scrambled_aucs = []\n","            for train_idx, val_idx in cv.split(X_train_full, y_scrambled):\n","                X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]\n","                y_train, y_val = y_scrambled[train_idx], y_scrambled[val_idx]\n","\n","                clf_scrambled = RidgeClassifier()\n","                clf_scrambled.fit(X_train, y_train)\n","                y_pred_scrambled = clf_scrambled.predict(X_val)\n","                auc = roc_auc_score(y_val, y_pred_scrambled)\n","                scrambled_aucs.append(auc)\n","\n","            print(f\"üîÄ Y-Scrambled ROC-AUC: {np.mean(scrambled_aucs):.4f} ¬± {np.std(scrambled_aucs):.4f}\")\n","            print(\"üëâ This should be near 0.5 if your real model learned something.\")\n","else: print('Ridge model not chosen')"]},{"cell_type":"markdown","metadata":{},"source":["## XGBoosted"]},{"cell_type":"code","execution_count":209,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":101704,"status":"ok","timestamp":1743547169212,"user":{"displayName":"Jeff","userId":"15773939950998775573"},"user_tz":-120},"id":"dvQ6gYSmeMsx","outputId":"9e230e2d-17e6-48b8-c75a-c17026d7c36f"},"outputs":[{"name":"stdout","output_type":"stream","text":["XGBoost model chosen\n","algpred2 dataset chosen\n","‚úÖ Loaded: Train=(16120, 320), Test=(4030, 320)\n","\n","üìâ DummyClassifier (Stratified) on Training Set (CV):\n","\n","üìä Dummy ROC-AUC: 0.4991 ¬± 0.0000\n","\n","üöÄ 5-Fold Cross-Validation (XGBoost) on Training Set...\n","\n","üìÇ Fold 1 AUC: 0.9957\n","              precision    recall  f1-score   support\n","\n","           0     0.9818    0.9708    0.9763      1612\n","           1     0.9712    0.9820    0.9766      1612\n","\n","    accuracy                         0.9764      3224\n","   macro avg     0.9765    0.9764    0.9764      3224\n","weighted avg     0.9765    0.9764    0.9764      3224\n","\n","------\n","üìÇ Fold 2 AUC: 0.9972\n","              precision    recall  f1-score   support\n","\n","           0     0.9801    0.9764    0.9782      1612\n","           1     0.9765    0.9801    0.9783      1612\n","\n","    accuracy                         0.9783      3224\n","   macro avg     0.9783    0.9783    0.9783      3224\n","weighted avg     0.9783    0.9783    0.9783      3224\n","\n","------\n","üìÇ Fold 3 AUC: 0.9978\n","              precision    recall  f1-score   support\n","\n","           0     0.9838    0.9789    0.9813      1612\n","           1     0.9790    0.9839    0.9814      1612\n","\n","    accuracy                         0.9814      3224\n","   macro avg     0.9814    0.9814    0.9814      3224\n","weighted avg     0.9814    0.9814    0.9814      3224\n","\n","------\n","üìÇ Fold 4 AUC: 0.9958\n","              precision    recall  f1-score   support\n","\n","           0     0.9795    0.9789    0.9792      1612\n","           1     0.9789    0.9795    0.9792      1612\n","\n","    accuracy                         0.9792      3224\n","   macro avg     0.9792    0.9792    0.9792      3224\n","weighted avg     0.9792    0.9792    0.9792      3224\n","\n","------\n","üìÇ Fold 5 AUC: 0.9976\n","              precision    recall  f1-score   support\n","\n","           0     0.9855    0.9671    0.9762      1612\n","           1     0.9677    0.9857    0.9766      1612\n","\n","    accuracy                         0.9764      3224\n","   macro avg     0.9766    0.9764    0.9764      3224\n","weighted avg     0.9766    0.9764    0.9764      3224\n","\n","------\n","\n","‚úÖ Mean CV ROC-AUC: 0.9968 ¬± 0.0010 (SE = 0.0004)\n","\n","üîí Final Evaluation on Hold-Out Test Set...\n","\n","              precision    recall  f1-score   support\n","\n","           0     0.7369    0.9811    0.8416      2015\n","           1     0.9718    0.6496    0.7787      2015\n","\n","    accuracy                         0.8154      4030\n","   macro avg     0.8543    0.8154    0.8102      4030\n","weighted avg     0.8543    0.8154    0.8102      4030\n","\n","üéØ Final Test ROC-AUC: 0.9477\n","\n","üß™ Y-Scrambling (sanity check) on Training Set...\n","\n","üîÄ Y-Scrambled ROC-AUC: 0.4974 ¬± 0.0093\n","üëâ This should be near 0.5 if your real model learned something.\n"]}],"source":["if model_name == 'XGBoost':\n","    # === Create log file path ===\n","    log_filename = f\"{model_name}_{transformer_name}_{dataset_name}_output.txt\"\n","    log_path = os.path.join(\"logs\", log_filename)\n","    os.makedirs(\"logs\", exist_ok=True)\n","\n","    class Tee:\n","        def __init__(self, *streams):\n","            self.streams = streams\n","\n","        def write(self, text):\n","            for stream in self.streams:\n","                stream.write(text)\n","                stream.flush()\n","\n","        def flush(self):\n","            for stream in self.streams:\n","                stream.flush()\n","\n","    with open(log_path, \"w\") as log_file:\n","        tee = Tee(sys.stdout, log_file)\n","        with redirect_stdout(tee), redirect_stderr(tee):\n","\n","            print('XGBoost model chosen')\n","            print(dataset_name, 'dataset chosen')\n","\n","            # Step 1: Load Data\n","            data_dir = dataset_name\n","            embedding_files = {\n","                \"train\": os.path.join(data_dir, f\"train_{dataset_name}_esm2_embeddings.csv\"),\n","                \"test\": os.path.join(data_dir, f\"test_{dataset_name}_esm2_embeddings.csv\")\n","            }\n","\n","            df_train = pd.read_csv(embedding_files[\"train\"])\n","            df_test = pd.read_csv(embedding_files[\"test\"])\n","\n","            feature_cols = [f\"f{i}\" for i in range(320)]\n","            X_train_full = df_train[feature_cols].values\n","            y_train_full = df_train[\"label\"].values\n","            X_test = df_test[feature_cols].values\n","            y_test = df_test[\"label\"].values\n","\n","            print(f\"‚úÖ Loaded: Train={X_train_full.shape}, Test={X_test.shape}\")\n","\n","            # Step 2: DummyClassifier\n","            print(\"\\nüìâ DummyClassifier (Stratified) on Training Set (CV):\\n\")\n","            dummy = DummyClassifier(strategy=\"stratified\", random_state=42)\n","            dummy_aucs = []\n","            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","            for train_idx, val_idx in cv.split(X_train_full, y_train_full):\n","                dummy.fit(X_train_full[train_idx], y_train_full[train_idx])\n","                y_dummy_proba = dummy.predict_proba(X_train_full[val_idx])[:, 1]\n","                auc = roc_auc_score(y_train_full[val_idx], y_dummy_proba)\n","                dummy_aucs.append(auc)\n","\n","            print(f\"üìä Dummy ROC-AUC: {np.mean(dummy_aucs):.4f} ¬± {np.std(dummy_aucs):.4f}\")\n","\n","            # Step 3: CV XGBoost\n","            print(\"\\nüöÄ 5-Fold Cross-Validation (XGBoost) on Training Set...\\n\")\n","            xgb_aucs = []\n","            for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_full, y_train_full)):\n","                X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]\n","                y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n","\n","                clf = xgb.XGBClassifier(eval_metric=\"logloss\", random_state=42)\n","                clf.fit(X_train, y_train)\n","\n","                y_pred = clf.predict(X_val)\n","                y_proba = clf.predict_proba(X_val)[:, 1]\n","\n","                auc = roc_auc_score(y_val, y_proba)\n","                xgb_aucs.append(auc)\n","\n","                print(f\"üìÇ Fold {fold+1} AUC: {auc:.4f}\")\n","                print(classification_report(y_val, y_pred, digits=4))\n","                print(\"------\")\n","\n","            mean_auc = np.mean(xgb_aucs)\n","            std_auc = np.std(xgb_aucs, ddof=1)\n","            se_auc = std_auc / np.sqrt(len(xgb_aucs))\n","            print(f\"\\n‚úÖ Mean CV ROC-AUC: {mean_auc:.4f} ¬± {std_auc:.4f} (SE = {se_auc:.4f})\")\n","\n","            # Step 4: Final Test\n","            print(\"\\nüîí Final Evaluation on Hold-Out Test Set...\\n\")\n","            clf_final = xgb.XGBClassifier(eval_metric=\"logloss\", random_state=42)\n","            clf_final.fit(X_train_full, y_train_full)\n","\n","            y_test_pred = clf_final.predict(X_test)\n","            y_test_proba = clf_final.predict_proba(X_test)[:, 1]\n","\n","            test_auc = roc_auc_score(y_test, y_test_proba)\n","            print(classification_report(y_test, y_test_pred, digits=4))\n","            print(f\"üéØ Final Test ROC-AUC: {test_auc:.4f}\")\n","\n","            # Step 5: Y-Scrambling\n","            print(\"\\nüß™ Y-Scrambling (sanity check) on Training Set...\\n\")\n","            y_scrambled = y_train_full.copy()\n","            random.seed(42)\n","            random.shuffle(y_scrambled)\n","\n","            scrambled_aucs = []\n","            for train_idx, val_idx in cv.split(X_train_full, y_scrambled):\n","                X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]\n","                y_train, y_val = y_scrambled[train_idx], y_scrambled[val_idx]\n","\n","                clf_scrambled = xgb.XGBClassifier(eval_metric=\"logloss\", random_state=42)\n","                clf_scrambled.fit(X_train, y_train)\n","                y_proba_scrambled = clf_scrambled.predict_proba(X_val)[:, 1]\n","\n","                auc = roc_auc_score(y_val, y_proba_scrambled)\n","                scrambled_aucs.append(auc)\n","\n","            print(f\"üîÄ Y-Scrambled ROC-AUC: {np.mean(scrambled_aucs):.4f} ¬± {np.std(scrambled_aucs):.4f}\")\n","            print(\"üëâ This should be near 0.5 if your real model learned something.\")\n","else:\n","    print('XGBoosted model not chosen')\n"]},{"cell_type":"markdown","metadata":{},"source":["## FeedForwardNeuralNetwork"]},{"cell_type":"code","execution_count":210,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["FFNN model not chosen\n"]}],"source":["if model_name == 'FFNN':\n","    # Define NN model\n","    class ProteinMLP(nn.Module):\n","        def __init__(self):\n","            super().__init__()\n","            self.net = nn.Sequential(\n","                nn.Linear(320, 256),\n","                nn.ReLU(),\n","                nn.Dropout(0.3),\n","                nn.Linear(256, 128),\n","                nn.ReLU(),\n","                nn.Dropout(0.3),\n","                nn.Linear(128, 1),\n","                nn.Sigmoid()\n","            )\n","\n","        def forward(self, x):\n","            return self.net(x)\n","    # === Create log file path ===\n","    log_filename = f\"{model_name}_{transformer_name}_{dataset_name}_output.txt\"\n","    log_path = os.path.join(\"logs\", log_filename)\n","    os.makedirs(\"logs\", exist_ok=True)\n","\n","    class Tee:\n","        def __init__(self, *streams):\n","            self.streams = streams\n","\n","        def write(self, text):\n","            for stream in self.streams:\n","                stream.write(text)\n","                stream.flush()\n","\n","        def flush(self):\n","            for stream in self.streams:\n","                stream.flush()\n","\n","    with open(log_path, \"w\") as log_file:\n","        tee = Tee(sys.stdout, log_file)\n","        with redirect_stdout(tee), redirect_stderr(tee):\n","\n","            print('FFNN model chosen')\n","   \n","            print(dataset_name, 'dataset chosen')\n","   \n","\n","            # ====================================\n","            # Step 1: Load Data\n","            # ====================================\n","            data_dir = dataset_name\n","            embedding_files = {\n","                \"train\": os.path.join(data_dir, f\"train_{dataset_name}_esm2_embeddings.csv\"),\n","                \"test\": os.path.join(data_dir, f\"test_{dataset_name}_esm2_embeddings.csv\")\n","            }\n","\n","            df_train = pd.read_csv(embedding_files[\"train\"])\n","            df_test = pd.read_csv(embedding_files[\"test\"])\n","\n","            feature_cols = [f\"f{i}\" for i in range(320)]\n","            X_train_full = df_train[feature_cols].values.astype(np.float32)\n","            y_train_full = df_train[\"label\"].values.astype(np.float32)\n","            X_test = df_test[feature_cols].values.astype(np.float32)\n","            y_test = df_test[\"label\"].values.astype(np.float32)\n","\n","            print(f\"‚úÖ Loaded: Train={X_train_full.shape}, Test={X_test.shape}\")\n","\n","            # ====================================\n","            # Step 2: DummyClassifier Baseline\n","            # ====================================\n","            print(\"\\nüìâ DummyClassifier (Stratified) on Training Set (CV):\\n\")\n","            dummy = DummyClassifier(strategy=\"stratified\", random_state=42)\n","            dummy_aucs = []\n","            cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","            for train_idx, val_idx in cv.split(X_train_full, y_train_full):\n","                dummy.fit(X_train_full[train_idx], y_train_full[train_idx])\n","                y_val = y_train_full[val_idx]\n","                y_dummy_proba = dummy.predict_proba(X_train_full[val_idx])\n","                if y_dummy_proba.shape[1] == 2:\n","                    y_dummy_proba = y_dummy_proba[:, 1]\n","                else:\n","                    y_dummy_proba = np.zeros_like(y_val)\n","                auc = roc_auc_score(y_val, y_dummy_proba)\n","                dummy_aucs.append(auc)\n","\n","            print(f\"üìä Dummy ROC-AUC: {np.mean(dummy_aucs):.4f} ¬± {np.std(dummy_aucs):.4f}\")\n","\n","            # ====================================\n","            # Step 3: 5-Fold CV with NN\n","            # ====================================\n","            print(\"\\nüöÄ 5-Fold Cross-Validation (NN) on Training Set...\\n\")\n","            nn_aucs = []\n","            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","            for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_full, y_train_full)):\n","                X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]\n","                y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n","\n","                train_ds = TensorDataset(torch.tensor(X_train), torch.tensor(y_train).unsqueeze(1))\n","                val_ds = TensorDataset(torch.tensor(X_val), torch.tensor(y_val).unsqueeze(1))\n","                train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n","                val_loader = DataLoader(val_ds, batch_size=64)\n","\n","                model = ProteinMLP().to(device)\n","                criterion = nn.BCELoss()\n","                optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","                # Training loop\n","                model.train()\n","                for epoch in range(10):\n","                    for xb, yb in train_loader:\n","                        xb, yb = xb.to(device), yb.to(device)\n","                        optimizer.zero_grad()\n","                        preds = model(xb)\n","                        loss = criterion(preds, yb)\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # Validation\n","                model.eval()\n","                all_preds = []\n","                with torch.no_grad():\n","                    for xb, _ in val_loader:\n","                        xb = xb.to(device)\n","                        preds = model(xb).cpu().numpy()\n","                        all_preds.extend(preds)\n","                preds_bin = (np.array(all_preds) > 0.5).astype(int)\n","\n","                auc = roc_auc_score(y_val, np.array(all_preds))\n","                nn_aucs.append(auc)\n","\n","                print(f\"üìÇ Fold {fold+1} AUC: {auc:.4f}\")\n","                print(classification_report(y_val, preds_bin, digits=4))\n","                print(\"------\")\n","            print(f\"\\n‚úÖ Mean CV ROC-AUC: {np.mean(nn_aucs):.4f} ¬± {np.std(nn_aucs):.4f}\")\n","\n","            # ====================================\n","            # Step 4: Final Test Set Evaluation\n","            # ====================================\n","            print(\"\\nüîí Final Evaluation on Hold-Out Test Set...\\n\")\n","            train_ds = TensorDataset(torch.tensor(X_train_full), torch.tensor(y_train_full).unsqueeze(1))\n","            test_ds = TensorDataset(torch.tensor(X_test), torch.tensor(y_test).unsqueeze(1))\n","            train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n","            test_loader = DataLoader(test_ds, batch_size=64)\n","\n","            final_model = ProteinMLP().to(device)\n","            optimizer = optim.Adam(final_model.parameters(), lr=1e-3)\n","            criterion = nn.BCELoss()\n","\n","            final_model.train()\n","            for epoch in range(20):\n","                for xb, yb in train_loader:\n","                    xb, yb = xb.to(device), yb.to(device)\n","                    optimizer.zero_grad()\n","                    preds = final_model(xb)\n","                    loss = criterion(preds, yb)\n","                    loss.backward()\n","                    optimizer.step()\n","\n","            final_model.eval()\n","            y_test_preds, y_test_probs = [], []\n","            with torch.no_grad():\n","                for xb, _ in test_loader:\n","                    xb = xb.to(device)\n","                    probs = final_model(xb).cpu().numpy()\n","                    preds = (probs > 0.5).astype(int)\n","                    y_test_preds.extend(preds)\n","                    y_test_probs.extend(probs)\n","\n","            test_auc = roc_auc_score(y_test, y_test_probs)\n","            print(classification_report(y_test, y_test_preds, digits=4))\n","            print(f\"üéØ Final Test ROC-AUC: {test_auc:.4f}\")\n","else: print('FFNN model not chosen')"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO8zzovwpYg5elCgDfk9yWE","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.21"}},"nbformat":4,"nbformat_minor":0}
