{"cells":[{"cell_type":"markdown","metadata":{},"source":["## imports"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import requests\n","from io import StringIO\n","import torch\n","import esm\n","from tqdm import tqdm\n","import os\n","import csv\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","from sklearn.metrics import roc_auc_score, classification_report\n","from sklearn.dummy import DummyClassifier\n","import xgboost as xgb\n","import random\n","import zipfile\n","import io"]},{"cell_type":"markdown","metadata":{},"source":["# Data curation"]},{"cell_type":"markdown","metadata":{},"source":["## User input: choose which dataset to evaluate "]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Choose between 'benchmark' or 'algpred2' for now only those two\n","#dataset_name = \"benchmark\"\n","dataset_name = \"algpred2\" "]},{"cell_type":"markdown","metadata":{},"source":["## Benchmark data"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["if dataset_name == \"benchmark\":\n","    # Define base folder and output paths\n","    data_dir = dataset_name\n","    os.makedirs(data_dir, exist_ok=True)\n","\n","    train_csv_path = os.path.join(data_dir, f\"{dataset_name}_train.csv\")\n","    test_csv_path = os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","\n","    # Check for existing files\n","    if os.path.exists(train_csv_path) or os.path.exists(test_csv_path):\n","        user_input = input(f\"‚ö†Ô∏è Files already exist in '{data_dir}/'. Do you want to overwrite them? (y/n): \").strip().lower()\n","        if user_input != \"y\":\n","            print(\"‚è≠Ô∏è  Skipping FASTA parsing and CSV generation.\")\n","        else:\n","            proceed = True\n","    else:\n","        proceed = True\n","\n","    if 'proceed' in locals() and proceed:\n","        # Define file-label mapping and GitHub raw URLs\n","        base_url = \"https://raw.githubusercontent.com/Jeffateth/AllergenPredict/7fafbea0ab1646796abe40cafb800c46ba842bda/Benchmark_dataset\"\n","\n","        datasets = {\n","            \"train_p.fasta\": (1, \"train\"),\n","            \"train_n.fasta\": (0, \"train\"),\n","            \"test_p.fasta\":  (1, \"test\"),\n","            \"test_n.fasta\":  (0, \"test\")\n","        }\n","\n","        # Parse FASTA format\n","        def parse_fasta(fasta_text, label):\n","            sequences = []\n","            current_id = None\n","            current_seq = \"\"\n","            for line in fasta_text.strip().splitlines():\n","                line = line.strip()\n","                if line.startswith(\">\"):\n","                    if current_id is not None:\n","                        sequences.append((current_id, current_seq, label))\n","                    current_id = line[1:]\n","                    current_seq = \"\"\n","                else:\n","                    current_seq += line\n","            if current_id and current_seq:\n","                sequences.append((current_id, current_seq, label))\n","            return sequences\n","\n","        # Download and parse files\n","        train_entries = []\n","        test_entries = []\n","\n","        for filename, (label, split) in datasets.items():\n","            url = f\"{base_url}/{filename}\"\n","            print(f\"‚¨áÔ∏è  Downloading {filename} from {url}...\")\n","            response = requests.get(url)\n","            response.raise_for_status()  # raise an error for failed downloads\n","\n","            fasta_text = response.text\n","            entries = parse_fasta(fasta_text, label)\n","            if split == \"train\":\n","                train_entries.extend(entries)\n","            else:\n","                test_entries.extend(entries)\n","\n","        # Save to CSV inside dataset-named folder\n","        df_train = pd.DataFrame(train_entries, columns=[\"id\", \"sequence\", \"label\"])\n","        df_test = pd.DataFrame(test_entries, columns=[\"id\", \"sequence\", \"label\"])\n","\n","        df_train.to_csv(train_csv_path, index=False)\n","        df_test.to_csv(test_csv_path, index=False)\n","\n","        print(f\"‚úÖ Saved training set to '{train_csv_path}'\")\n","        print(f\"‚úÖ Saved testing set to '{test_csv_path}'\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data from AllergenAI (need to update this code to fit)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["# # Standard amino acid order (1-letter codes)\n","# aa_letters = list(\"ACDEFGHIKLMNPQRSTVWY\")\n","\n","# # Map one-hot vector to amino acid letter\n","# onehot_to_aa = {\n","#     tuple(1 if i == j else 0 for i in range(20)): aa\n","#     for j, aa in enumerate(aa_letters)\n","# }\n","\n","# def load_onehot_file(filepath, label):\n","#     \"\"\"Converts one-hot file to list of (sequence, label)\"\"\"\n","#     data = np.loadtxt(filepath)\n","#     sequences = []\n","#     current = []\n","\n","#     for row in data:\n","#         if np.all(row == 0):\n","#             if current:\n","#                 sequences.append((\"\".join(current), label))\n","#                 current = []\n","#         else:\n","#             aa = onehot_to_aa.get(tuple(int(x) for x in row))\n","#             if aa:\n","#                 current.append(aa)\n","#             else:\n","#                 raise ValueError(f\"Unknown one-hot vector: {row}\")\n","\n","#     if current:\n","#         sequences.append((\"\".join(current), label))\n","\n","#     return sequences\n","\n","# # === Load both files ===\n","# positive_sequences = load_onehot_file(\"pos.txt\", label=1)\n","# negative_sequences = load_onehot_file(\"neg.txt\", label=0)\n","\n","# # === Combine and save ===\n","# all_sequences = positive_sequences + negative_sequences\n","# df = pd.DataFrame(all_sequences, columns=[\"sequence\", \"label\"])\n","# df[\"id\"] = range(len(df))\n","\n","# # Reorder columns if you want: id, sequence, label\n","# df = df[[\"id\", \"sequence\", \"label\"]]\n","\n","# # Save it back\n","# df.to_csv(\"converted_onehot_sequences.csv\", index=False)\n","# print(\"‚úÖ Saved as 'converted_onehot_sequences.csv'\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## Data from AlgPred 2.0"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9444,"status":"ok","timestamp":1743545518910,"user":{"displayName":"Jeff","userId":"15773939950998775573"},"user_tz":-120},"id":"ilcsg8tr0Nus","outputId":"6b11ab8a-9c74-4ba9-f7fb-5b59a4090162"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚¨áÔ∏è  Downloading train_positive from https://webs.iiitd.edu.in/raghava/algpred2/datasets/train_positive.txt...\n","‚¨áÔ∏è  Downloading train_negative from https://webs.iiitd.edu.in/raghava/algpred2/datasets/train_negative.txt...\n","‚¨áÔ∏è  Downloading validation_positive from https://webs.iiitd.edu.in/raghava/algpred2/datasets/validation_positive.txt...\n","‚¨áÔ∏è  Downloading validation_negative from https://webs.iiitd.edu.in/raghava/algpred2/datasets/validation_negative.txt...\n","‚úÖ Saved training set to 'algpred2/algpred2_train.csv'\n","‚úÖ Saved validation set to 'algpred2/algpred2_test.csv'\n"]}],"source":["if dataset_name == \"algpred2\":\n","    # Define output paths inside dataset-named directory\n","    data_dir = dataset_name\n","    os.makedirs(data_dir, exist_ok=True)\n","\n","    train_csv_path = os.path.join(data_dir, f\"{dataset_name}_train.csv\")\n","    test_csv_path = os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","\n","    # Check if files already exist\n","    if os.path.exists(train_csv_path) or os.path.exists(test_csv_path):\n","        user_input = input(f\"‚ö†Ô∏è Files already exist in '{data_dir}/'. Do you want to overwrite them? (y/n): \").strip().lower()\n","        if user_input != \"y\":\n","            print(\"‚è≠Ô∏è  Skipping download and parsing.\")\n","        else:\n","            proceed = True\n","    else:\n","        proceed = True\n","\n","    if 'proceed' in locals() and proceed:\n","        # URLs from AlgPred 2.0\n","        datasets = {\n","            \"train_positive\": (\"https://webs.iiitd.edu.in/raghava/algpred2/datasets/train_positive.txt\", 1, \"train\"),\n","            \"train_negative\": (\"https://webs.iiitd.edu.in/raghava/algpred2/datasets/train_negative.txt\", 0, \"train\"),\n","            \"validation_positive\": (\"https://webs.iiitd.edu.in/raghava/algpred2/datasets/validation_positive.txt\", 1, \"val\"),\n","            \"validation_negative\": (\"https://webs.iiitd.edu.in/raghava/algpred2/datasets/validation_negative.txt\", 0, \"val\")\n","        }\n","\n","        # Parse FASTA format\n","        def parse_fasta(fasta_text, label):\n","            sequences = []\n","            current_id = None\n","            current_seq = \"\"\n","            for line in fasta_text.strip().splitlines():\n","                line = line.strip()\n","                if line.startswith(\">\"):\n","                    if current_id is not None:\n","                        sequences.append((current_id, current_seq, label))\n","                    current_id = line[1:]  # remove \">\"\n","                    current_seq = \"\"\n","                else:\n","                    current_seq += line\n","            if current_id and current_seq:\n","                sequences.append((current_id, current_seq, label))\n","            return sequences\n","\n","        # Split into train and validation entries\n","        train_entries = []\n","        val_entries = []\n","\n","        for name, (url, label, split) in datasets.items():\n","            print(f\"‚¨áÔ∏è  Downloading {name} from {url}...\")\n","            response = requests.get(url)\n","            entries = parse_fasta(response.text, label)\n","            if split == \"train\":\n","                train_entries.extend(entries)\n","            else:\n","                val_entries.extend(entries)\n","\n","        # Convert to DataFrames\n","        df_train = pd.DataFrame(train_entries, columns=[\"id\", \"sequence\", \"label\"])\n","        df_val = pd.DataFrame(val_entries, columns=[\"id\", \"sequence\", \"label\"])\n","\n","        # Save to CSV inside dataset folder\n","        df_train.to_csv(train_csv_path, index=False)\n","        df_val.to_csv(test_csv_path, index=False)\n","\n","        print(f\"‚úÖ Saved training set to '{train_csv_path}'\")\n","        print(f\"‚úÖ Saved validation set to '{test_csv_path}'\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# ESM-2 embedding extraction"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":206837,"status":"ok","timestamp":1743546270778,"user":{"displayName":"Jeff","userId":"15773939950998775573"},"user_tz":-120},"id":"1_Jv62ty0mrA","outputId":"778a1e22-2458-4988-a7f1-e5b0c49097ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚öôÔ∏è  Extracting embeddings for train set... (16120 sequences remaining)\n"]},{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16120/16120 [25:20<00:00, 10.60it/s]  \n"]},{"name":"stdout","output_type":"stream","text":["‚úÖ Final train embeddings saved to 'algpred2/train_algpred2_esm2_embeddings.csv'\n","‚öôÔ∏è  Extracting embeddings for test set... (4030 sequences remaining)\n"]},{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4030/4030 [06:08<00:00, 10.93it/s]"]},{"name":"stdout","output_type":"stream","text":["‚úÖ Final test embeddings saved to 'algpred2/test_algpred2_esm2_embeddings.csv'\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# === CONFIG ===\n","feature_dim = 320           # ESM-2 T6-8M embedding size\n","batch_size = 1              # Adjust based on memory\n","data_dir = dataset_name     # All files live in a folder named after the dataset\n","\n","# --- Ensure directory exists ---\n","os.makedirs(data_dir, exist_ok=True)\n","\n","# --- Construct dynamic file paths ---\n","input_files = {\n","    \"train\": os.path.join(data_dir, f\"{dataset_name}_train.csv\"),\n","    \"test\": os.path.join(data_dir, f\"{dataset_name}_test.csv\")\n","}\n","\n","# --- Output file paths ---\n","embedding_files = {\n","    \"train\": os.path.join(data_dir, f\"train_{dataset_name}_esm2_embeddings.csv\"),\n","    \"test\": os.path.join(data_dir, f\"test_{dataset_name}_esm2_embeddings.csv\")\n","}\n","\n","# Check if both embedding files exist\n","if all(os.path.exists(f) for f in embedding_files.values()):\n","    print(f\"‚úÖ ESM2 embedding files already exist in '{data_dir}/'. Skipping embedding generation.\")\n","else:\n","    # --- Load ESM-2 model ---\n","    model, alphabet = esm.pretrained.esm2_t6_8M_UR50D()\n","    batch_converter = alphabet.get_batch_converter()\n","    model.eval()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","\n","    # --- Helper function ---\n","    def process_file(split_name, input_file):\n","        temp_file = os.path.join(data_dir, f\"{split_name}_{dataset_name}_esm2_embeddings_temp.csv\")\n","        final_file = os.path.join(data_dir, f\"{split_name}_{dataset_name}_esm2_embeddings.csv\")\n","\n","        # Load dataset\n","        df = pd.read_csv(input_file)\n","        sequences = list(df[\"sequence\"])\n","        labels = list(df[\"label\"])\n","        ids = list(df[\"id\"])\n","\n","        # Resume support\n","        if os.path.exists(temp_file):\n","            processed_ids = set(pd.read_csv(temp_file, usecols=[\"id\"])[\"id\"])\n","            print(f\"üîÅ Resuming {split_name} from {temp_file} ‚Äî {len(processed_ids)} entries already processed.\")\n","        else:\n","            processed_ids = set()\n","\n","        remaining_data = [(ids[i], sequences[i], labels[i]) for i in range(len(ids)) if ids[i] not in processed_ids]\n","\n","        # Output format\n","        fieldnames = [\"id\", \"label\"] + [f\"f{k}\" for k in range(feature_dim)]\n","        write_header = not os.path.exists(temp_file)\n","\n","        print(f\"‚öôÔ∏è  Extracting embeddings for {split_name} set... ({len(remaining_data)} sequences remaining)\")\n","\n","        with open(temp_file, mode=\"a\", newline=\"\") as f:\n","            writer = csv.DictWriter(f, fieldnames=fieldnames)\n","            if write_header:\n","                writer.writeheader()\n","\n","            for i in tqdm(range(0, len(remaining_data), batch_size)):\n","                batch = remaining_data[i:i + batch_size]\n","                batch_ids = [x[0] for x in batch]\n","                batch_seqs = [x[1] for x in batch]\n","                batch_labels = [x[2] for x in batch]\n","\n","                batch_data = [(batch_ids[j], batch_seqs[j]) for j in range(len(batch_seqs))]\n","                _, _, batch_tokens = batch_converter(batch_data)\n","                batch_tokens = batch_tokens.to(device)\n","\n","                with torch.no_grad():\n","                    outputs = model(batch_tokens, repr_layers=[6])\n","                    token_representations = outputs[\"representations\"][6]\n","\n","                rows = []\n","                for j, (_, seq) in enumerate(batch_data):\n","                    representation = token_representations[j, 1:len(seq)+1].mean(0)\n","                    entry = {\n","                        \"id\": batch_ids[j],\n","                        \"label\": batch_labels[j],\n","                    }\n","                    for k in range(feature_dim):\n","                        entry[f\"f{k}\"] = representation[k].item()\n","                    rows.append(entry)\n","\n","                writer.writerows(rows)\n","\n","        # Final save\n","        os.replace(temp_file, final_file)\n","        print(f\"‚úÖ Final {split_name} embeddings saved to '{final_file}'\")\n","\n","    # --- Process each split ---\n","    for split, file in input_files.items():\n","        process_file(split, file)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Model (XGBoosted)"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":101704,"status":"ok","timestamp":1743547169212,"user":{"displayName":"Jeff","userId":"15773939950998775573"},"user_tz":-120},"id":"dvQ6gYSmeMsx","outputId":"9e230e2d-17e6-48b8-c75a-c17026d7c36f"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚úÖ Loaded: Train=(16120, 320), Test=(4030, 320)\n","\n","üìâ DummyClassifier (Stratified) on Training Set (CV):\n","\n","üìä Dummy ROC-AUC: 0.4991 ¬± 0.0000\n","\n","üöÄ 5-Fold Cross-Validation (XGBoost) on Training Set...\n","\n"]},{"name":"stderr","output_type":"stream","text":["/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n","  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"]},{"name":"stdout","output_type":"stream","text":["üìÇ Fold 1 AUC: 0.9957\n","              precision    recall  f1-score   support\n","\n","           0     0.9818    0.9708    0.9763      1612\n","           1     0.9712    0.9820    0.9766      1612\n","\n","    accuracy                         0.9764      3224\n","   macro avg     0.9765    0.9764    0.9764      3224\n","weighted avg     0.9765    0.9764    0.9764      3224\n","\n","------\n"]},{"name":"stderr","output_type":"stream","text":["/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n","  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"]},{"name":"stdout","output_type":"stream","text":["üìÇ Fold 2 AUC: 0.9972\n","              precision    recall  f1-score   support\n","\n","           0     0.9801    0.9764    0.9782      1612\n","           1     0.9765    0.9801    0.9783      1612\n","\n","    accuracy                         0.9783      3224\n","   macro avg     0.9783    0.9783    0.9783      3224\n","weighted avg     0.9783    0.9783    0.9783      3224\n","\n","------\n"]},{"name":"stderr","output_type":"stream","text":["/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n","  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"]},{"name":"stdout","output_type":"stream","text":["üìÇ Fold 3 AUC: 0.9978\n","              precision    recall  f1-score   support\n","\n","           0     0.9838    0.9789    0.9813      1612\n","           1     0.9790    0.9839    0.9814      1612\n","\n","    accuracy                         0.9814      3224\n","   macro avg     0.9814    0.9814    0.9814      3224\n","weighted avg     0.9814    0.9814    0.9814      3224\n","\n","------\n"]},{"name":"stderr","output_type":"stream","text":["/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n","  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"]},{"name":"stdout","output_type":"stream","text":["üìÇ Fold 4 AUC: 0.9958\n","              precision    recall  f1-score   support\n","\n","           0     0.9795    0.9789    0.9792      1612\n","           1     0.9789    0.9795    0.9792      1612\n","\n","    accuracy                         0.9792      3224\n","   macro avg     0.9792    0.9792    0.9792      3224\n","weighted avg     0.9792    0.9792    0.9792      3224\n","\n","------\n"]},{"name":"stderr","output_type":"stream","text":["/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n","  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"]},{"name":"stdout","output_type":"stream","text":["üìÇ Fold 5 AUC: 0.9976\n","              precision    recall  f1-score   support\n","\n","           0     0.9855    0.9671    0.9762      1612\n","           1     0.9677    0.9857    0.9766      1612\n","\n","    accuracy                         0.9764      3224\n","   macro avg     0.9766    0.9764    0.9764      3224\n","weighted avg     0.9766    0.9764    0.9764      3224\n","\n","------\n","\n","‚úÖ Mean CV ROC-AUC: 0.9968 ¬± 0.0009\n","\n","üîí Final Evaluation on Hold-Out Test Set...\n","\n"]},{"name":"stderr","output_type":"stream","text":["/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n","  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0     0.7369    0.9811    0.8416      2015\n","           1     0.9718    0.6496    0.7787      2015\n","\n","    accuracy                         0.8154      4030\n","   macro avg     0.8543    0.8154    0.8102      4030\n","weighted avg     0.8543    0.8154    0.8102      4030\n","\n","üéØ Final Test ROC-AUC: 0.9477\n","\n","üß™ Y-Scrambling (sanity check) on Training Set...\n","\n"]},{"name":"stderr","output_type":"stream","text":["/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n","  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n","/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n","  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n","/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n","  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n","/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n","  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n","/opt/miniconda3/envs/esm2_env/lib/python3.9/site-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n","  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"]},{"name":"stdout","output_type":"stream","text":["üîÄ Y-Scrambled ROC-AUC: 0.4974 ¬± 0.0093\n","üëâ This should be near 0.5 if your real model learned something.\n"]}],"source":["# ====================================\n","# Step 1: Load Data\n","# ====================================\n","# Set the dataset name\n","\n","# Construct embedding file paths based on dataset name\n","embedding_files = {\n","    \"train\": f\"train_{dataset_name}_esm2_embeddings.csv\",\n","    \"test\": f\"test_{dataset_name}_esm2_embeddings.csv\"\n","}\n","\n","# Load the data\n","df_train = pd.read_csv(embedding_files[\"train\"])\n","df_test = pd.read_csv(embedding_files[\"test\"])\n","\n","\n","feature_cols = [f\"f{i}\" for i in range(320)]\n","X_train_full = df_train[feature_cols].values\n","y_train_full = df_train[\"label\"].values\n","\n","X_test = df_test[feature_cols].values\n","y_test = df_test[\"label\"].values\n","\n","print(f\"‚úÖ Loaded: Train={X_train_full.shape}, Test={X_test.shape}\")\n","\n","# ====================================\n","# Step 2: Dummy Classifier Baseline (on Train)\n","# ====================================\n","print(\"\\nüìâ DummyClassifier (Stratified) on Training Set (CV):\\n\")\n","dummy = DummyClassifier(strategy=\"stratified\", random_state=42)\n","dummy_aucs = []\n","cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","for train_idx, val_idx in cv.split(X_train_full, y_train_full):\n","    dummy.fit(X_train_full[train_idx], y_train_full[train_idx])\n","    y_dummy_proba = dummy.predict_proba(X_train_full[val_idx])[:, 1]\n","    auc = roc_auc_score(y_train_full[val_idx], y_dummy_proba)\n","    dummy_aucs.append(auc)\n","\n","print(f\"üìä Dummy ROC-AUC: {np.mean(dummy_aucs):.4f} ¬± {np.std(dummy_aucs):.4f}\")\n","\n","# ====================================\n","# Step 3: Cross-Validation on Training Set (XGBoost)\n","# ====================================\n","print(\"\\nüöÄ 5-Fold Cross-Validation (XGBoost) on Training Set...\\n\")\n","xgb_aucs = []\n","\n","for fold, (train_idx, val_idx) in enumerate(cv.split(X_train_full, y_train_full)):\n","    X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]\n","    y_train, y_val = y_train_full[train_idx], y_train_full[val_idx]\n","\n","    clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n","    clf.fit(X_train, y_train)\n","\n","    y_pred = clf.predict(X_val)\n","    y_proba = clf.predict_proba(X_val)[:, 1]\n","\n","    auc = roc_auc_score(y_val, y_proba)\n","    xgb_aucs.append(auc)\n","\n","    print(f\"üìÇ Fold {fold+1} AUC: {auc:.4f}\")\n","    print(classification_report(y_val, y_pred, digits=4))\n","    print(\"------\")\n","\n","print(f\"\\n‚úÖ Mean CV ROC-AUC: {np.mean(xgb_aucs):.4f} ¬± {np.std(xgb_aucs):.4f}\")\n","\n","# ====================================\n","# Step 4: Final Test Set Evaluation\n","# ====================================\n","print(\"\\nüîí Final Evaluation on Hold-Out Test Set...\\n\")\n","clf_final = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n","clf_final.fit(X_train_full, y_train_full)\n","\n","y_test_pred = clf_final.predict(X_test)\n","y_test_proba = clf_final.predict_proba(X_test)[:, 1]\n","\n","test_auc = roc_auc_score(y_test, y_test_proba)\n","print(classification_report(y_test, y_test_pred, digits=4))\n","print(f\"üéØ Final Test ROC-AUC: {test_auc:.4f}\")\n","\n","# ====================================\n","# Step 5: Y-Scrambling Control\n","# ====================================\n","print(\"\\nüß™ Y-Scrambling (sanity check) on Training Set...\\n\")\n","y_scrambled = y_train_full.copy()\n","random.seed(42)\n","random.shuffle(y_scrambled)\n","\n","scrambled_aucs = []\n","for train_idx, val_idx in cv.split(X_train_full, y_scrambled):\n","    X_train, X_val = X_train_full[train_idx], X_train_full[val_idx]\n","    y_train, y_val = y_scrambled[train_idx], y_scrambled[val_idx]\n","\n","    clf_scrambled = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n","    clf_scrambled.fit(X_train, y_train)\n","    y_proba_scrambled = clf_scrambled.predict_proba(X_val)[:, 1]\n","\n","    auc = roc_auc_score(y_val, y_proba_scrambled)\n","    scrambled_aucs.append(auc)\n","\n","print(f\"üîÄ Y-Scrambled ROC-AUC: {np.mean(scrambled_aucs):.4f} ¬± {np.std(scrambled_aucs):.4f}\")\n","print(\"üëâ This should be near 0.5 if your real model learned something.\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyO8zzovwpYg5elCgDfk9yWE","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.21"}},"nbformat":4,"nbformat_minor":0}
